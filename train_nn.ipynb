{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"tutorial.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"KvrVB5o0HZOp","colab_type":"code","outputId":"d2db82e8-a55c-40aa-cca6-cb7c73e3a7fd","executionInfo":{"status":"ok","timestamp":1574100277776,"user_tz":-330,"elapsed":25050,"user":{"displayName":"Srihari Vemuru","photoUrl":"","userId":"13401425132614860256"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from datetime import datetime\n","from tqdm import tqdm, tqdm_notebook\n","%matplotlib inline\n","\n","from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n","from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n","from sklearn.impute import SimpleImputer\n","# from sklearn.metrics import roc_auc_score\n","# import lightgbm as lgb\n","# from numba import jit\n","# from sklearn.svm import SVC\n","# from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EB3LY5O9HZPG","colab_type":"code","colab":{}},"source":["dtypes = {\n","        'MachineIdentifier':                                    'object',\n","        'ProductName':                                          'category',\n","        'EngineVersion':                                        'category',\n","        'AppVersion':                                           'category',\n","        'AvSigVersion':                                         'category',\n","        'Platform':                                             'category',\n","        'Processor':                                            'category',\n","        'OsVer':                                                'category',\n","        'OsPlatformSubRelease':                                 'category',\n","        'OsBuildLab':                                           'category',\n","        'SkuEdition':                                           'category',\n","        'PuaMode':                                              'category',\n","        'SmartScreen':                                          'category',\n","        'Census_MDC2FormFactor':                                'category',\n","        'Census_DeviceFamily':                                  'category',\n","        'Census_ProcessorClass':                                'category',\n","        'Census_PrimaryDiskTypeName':                           'category',\n","        'Census_ChassisTypeName':                               'category',\n","        'Census_PowerPlatformRoleName':                         'category',\n","        'Census_InternalBatteryType':                           'category',\n","        'Census_OSVersion':                                     'category',\n","        'Census_OSArchitecture':                                'category',\n","        'Census_OSBranch':                                      'category',\n","        'Census_OSEdition':                                     'category',\n","        'Census_OSSkuName':                                     'category',\n","        'Census_OSInstallTypeName':                             'category',\n","        'Census_OSWUAutoUpdateOptionsName':                     'category',\n","        'Census_GenuineStateName':                              'category',\n","        'Census_ActivationChannel':                             'category',\n","        'Census_FlightRing':                                    'category'\n","}\n","\n","train = pd.read_csv('/content/gdrive/My Drive/ML_datasets/train.csv', dtype=dtypes)\n","# test = pd.read_csv('/content/gdrive/My Drive/ML_datasets/test.csv', dtype=dtypes)\n","target = train['HasDetections']\n","# machine_id = test['MachineIdentifier']\n","train.drop(['MachineIdentifier'], inplace=True, axis=1)\n","train.drop(['HasDetections'], inplace=True, axis=1)\n","# test.drop(['MachineIdentifier'], inplace=True, axis=1)\n","columns = train.columns"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"Rdl5IlRTHZPW","colab_type":"code","outputId":"d948867a-c0d1-4fc9-ae0d-6b2074ae5f6c","executionInfo":{"status":"ok","timestamp":1573880943973,"user_tz":-330,"elapsed":4486,"user":{"displayName":"Srihari Vemuru","photoUrl":"","userId":"13401425132614860256"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["%%time\n","# checking missing train data\n","total = train.isnull().sum().sort_values(ascending = False)\n","percent = (train.isnull().sum()/train.isnull().count()*100).sort_values(ascending = False)\n","missing_train_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n","missing_train_data.head(50)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CPU times: user 21.8 ms, sys: 51 µs, total: 21.9 ms\n","Wall time: 22.2 ms\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"T2LWHNIzHZPn","colab_type":"code","colab":{}},"source":["# def f(pt, train, cn):\n","#     l = []\n","#     for i in pt.index:\n","#         l.append((train[cn]==i).sum())\n","#     l = np.array(l)\n","#     l = l-pt['HasDetections'].to_numpy()\n","#     return l\n","# j = 1\n","# for i in rem_columns:\n","#     data = train[[i, 'HasDetections']]\n","#     pt = pd.pivot_table(data, values=['HasDetections'], index=[i], aggfunc=np.sum)\n","#     pt['Remaining'] = f(pt, train, i)\n","#     print(pt)\n","#     print(j, 'no of nulls: ', percent[i])\n","#     j += 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vyDlbttjHZP0","colab_type":"code","colab":{}},"source":["rem_columns = ['IsBeta', 'PuaMode', 'AutoSampleOptIn', 'DefaultBrowsersIdentifier', 'Census_IsWIMBootEnabled', 'Census_IsTouchEnabled', 'Census_IsPenCapable', 'Census_ThresholdOptIn', 'Census_IsFlightingInternal', 'Census_IsFlightsDisabled', 'Census_IsPortableOperatingSystem', 'SMode', 'Census_ProcessorClass']\n","train = train.drop(columns = rem_columns)\n","# test = test.drop(columns = rem_columns)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"utOMES63HZQA","colab_type":"code","colab":{}},"source":["# columns = train.columns\n","# cat_cols = []\n","# num_cols = []\n","# num_cat = [\n","# 'RtpStateBitfield', 'AVProductStatesIdentifier', \n","# 'CityIdentifier' , 'OrganizationIdentifier','GeoNameIdentifier', \n","# 'IeVerIdentifier', 'UacLuaenable', 'Census_OEMNameIdentifier', \n","# 'Census_OEMModelIdentifier', 'Census_ProcessorManufacturerIdentifier', \n","# 'Census_ProcessorModelIdentifier', 'Census_OSInstallLanguageIdentifier', \n","# 'Census_FirmwareManufacturerIdentifier', 'Census_FirmwareVersionIdentifier', \n","# 'Wdft_RegionIdentifier', 'OsBuild', 'OsSuite', \n","# 'Census_OSBuildNumber', 'CountryIdentifier', 'Census_OSUILocaleIdentifier', 'Census_OSBuildRevision'\n","# ]\n","# for i in columns:\n","#     if(str(train[i].dtype) == 'category'):\n","#         cat_cols.append(i)\n","#     else:\n","#         num_cols.append(i)\n","# num_non_cat = []\n","# for i in num_cols:\n","#     if(i not in num_cat):\n","#         num_non_cat.append(i)\n","        \n","# disected_cols = ['EngineVersion','AppVersion','AvSigVersion']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kWdq4OqtHZQJ","colab_type":"text"},"source":["### Spliting Few Columns"]},{"cell_type":"code","metadata":{"id":"vYeoXTBMHZQL","colab_type":"code","colab":{}},"source":["disected_cols = ['EngineVersion','AppVersion','AvSigVersion']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZPXDlcH4HZQU","colab_type":"code","colab":{}},"source":["train['OsBuildLab'] = train.OsBuildLab.astype(str)\n","for i in range(train.shape[0]):\n","    train['OsBuildLab'][i] = str(train['OsBuildLab'][i])[-11:-6]\n","    \n","os_build_lab = train.as_matrix(columns=['OsBuildLab'])\n","\n","os_build_lab_extracted = []\n","for i in range(os_build_lab.shape[0]):\n","    os_build_lab_extracted.append([str(os_build_lab[i])[2:4],str(os_build_lab[i])[4:6]])\n","os_build_lab_extracted = np.array(os_build_lab_extracted)\n","\n","train['OsBuildLabYear'] = os_build_lab_extracted[:,0]\n","train['OsBuildLabMonth'] = os_build_lab_extracted[:,1]\n","train['OsBuildLabYear'] = train['OsBuildLabYear'].astype('category')\n","train['OsBuildLabMonth'] = train['OsBuildLabMonth'].astype('category')\n","\n","train.drop(columns = ['OsBuildLab'],inplace = True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SJTuLSQraJoO","colab_type":"code","colab":{}},"source":["# test['OsBuildLab'] = test.OsBuildLab.astype(str)\n","# for i in range(test.shape[0]):\n","#     test['OsBuildLab'][i] = str(test['OsBuildLab'][i])[-11:-6]\n","    \n","# os_build_lab = test.as_matrix(columns=['OsBuildLab'])\n","\n","# os_build_lab_extracted = []\n","# for i in range(os_build_lab.shape[0]):\n","#     os_build_lab_extracted.append([str(os_build_lab[i])[2:4],str(os_build_lab[i])[4:6]])\n","# os_build_lab_extracted = np.array(os_build_lab_extracted)\n","\n","# test['OsBuildLabYear'] = os_build_lab_extracted[:,0]\n","# test['OsBuildLabMonth'] = os_build_lab_extracted[:,1]\n","# test['OsBuildLabYear'] = test['OsBuildLabYear'].astype('category')\n","# test['OsBuildLabMonth'] = test['OsBuildLabMonth'].astype('category')\n","\n","# test.drop(columns = ['OsBuildLab'],inplace = True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zgS73pWTHZQf","colab_type":"code","colab":{}},"source":["# os_build_lab_extracted = []\n","for i in disected_cols:\n","    matrix_extracted = []\n","    for j in range(train.shape[0]): \n","        matrix_extracted.append(str(train[i][j]).split(\".\"))\n","    matrix_extracted = np.array(matrix_extracted)    \n","    train[i + str(1)] = matrix_extracted[:,0]\n","    train[i + str(2)] = matrix_extracted[:,1]\n","    train[i + str(3)] = matrix_extracted[:,2]\n","    train[i + str(4)] = matrix_extracted[:,3]\n","    \n","    train[i + str(1)] = train[i + str(1)].astype('category')\n","    train[i + str(2)] = train[i + str(2)].astype('category')\n","    train[i + str(3)] = train[i + str(3)].astype('category')\n","    train[i + str(4)] = train[i + str(4)].astype('category')\n","train.drop(columns = disected_cols,inplace = True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IxGi475WaZxD","colab_type":"code","colab":{}},"source":["# # os_build_lab_extracted = []\n","# for i in disected_cols:\n","#     matrix_extracted = []\n","#     for j in range(test.shape[0]): \n","#         matrix_extracted.append(str(test[i][j]).split(\".\"))\n","#     matrix_extracted = np.array(matrix_extracted)    \n","#     test[i + str(1)] = matrix_extracted[:,0]\n","#     test[i + str(2)] = matrix_extracted[:,1]\n","#     test[i + str(3)] = matrix_extracted[:,2]\n","#     test[i + str(4)] = matrix_extracted[:,3]\n","    \n","#     test[i + str(1)] = test[i + str(1)].astype('category')\n","#     test[i + str(2)] = test[i + str(2)].astype('category')\n","#     test[i + str(3)] = test[i + str(3)].astype('category')\n","#     test[i + str(4)] = test[i + str(4)].astype('category')\n","# test.drop(columns = disected_cols,inplace = True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W4fqTV8WHZQo","colab_type":"code","colab":{}},"source":["l = ['EngineVersion1', 'EngineVersion2', 'AppVersion1', 'AvSigVersion1', 'AvSigVersion4']\n","train = train.drop(columns = l)\n","columns = train.columns"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JbTFCn5vbJ_E","colab_type":"code","colab":{}},"source":["# l = ['EngineVersion1', 'EngineVersion2', 'AppVersion1', 'AvSigVersion1', 'AvSigVersion4']\n","# test = test.drop(columns = l)\n","# columns = test.columns"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CHKsDyZSHZQ3","colab_type":"code","colab":{}},"source":["columns = train.columns\n","cat_cols = []\n","num_cols = []\n","num_cat = [\n","'RtpStateBitfield', 'AVProductStatesIdentifier', \n","'CityIdentifier' , 'OrganizationIdentifier','GeoNameIdentifier', \n","'IeVerIdentifier', 'UacLuaenable', 'Census_OEMNameIdentifier', \n","'Census_OEMModelIdentifier', 'Census_ProcessorManufacturerIdentifier', \n","'Census_ProcessorModelIdentifier', 'Census_OSInstallLanguageIdentifier', \n","'Census_FirmwareManufacturerIdentifier', 'Census_FirmwareVersionIdentifier', \n","'Wdft_RegionIdentifier', 'OsBuild', 'OsSuite', \n","'Census_OSBuildNumber', 'CountryIdentifier', 'Census_OSUILocaleIdentifier', 'Census_OSBuildRevision'\n","]\n","for i in columns:\n","    if(str(train[i].dtype) == 'category'):\n","        cat_cols.append(i)\n","    else:\n","        num_cols.append(i)\n","num_non_cat = []\n","for i in num_cols:\n","    if(i not in num_cat):\n","        num_non_cat.append(i)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"US4eIZbGHZRD","colab_type":"text"},"source":["### Non-Cat Imputing ###"]},{"cell_type":"code","metadata":{"id":"lUjvLwZLHZRG","colab_type":"code","colab":{}},"source":["def isnan(feature):\n","    return 1 if np.isnan(float(feature)) else 0\n","\n","train['core_nan'] = train['Census_ProcessorCoreCount'].apply(isnan).astype('uint8')\n","train['ram_nan'] = train['Census_TotalPhysicalRAM'].apply(isnan).astype('uint8')\n","train['primary_nan'] = train['Census_PrimaryDiskTotalCapacity'].apply(isnan).astype('uint8')\n","train['system_nan'] = train['Census_SystemVolumeTotalCapacity'].apply(isnan).astype('uint8')\n","train['diagonal_nan'] = train['Census_InternalPrimaryDiagonalDisplaySizeInInches'].apply(isnan).astype('uint8')\n","train['horizonal_nan'] = train['Census_InternalPrimaryDisplayResolutionHorizontal'].apply(isnan).astype('uint8')\n","train['vertical_nan'] = train['Census_InternalPrimaryDisplayResolutionVertical'].apply(isnan).astype('uint8')\n","train['charges_nan'] = train['Census_InternalBatteryNumberOfCharges'].apply(isnan).astype('uint8')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Db29u3v_HZRP","colab_type":"code","colab":{}},"source":["mean_imp = SimpleImputer(strategy='median')\n","\n","for i in num_non_cat:\n","    train[i] = mean_imp.fit_transform(train[i].values.reshape(-1,1))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"JfvVwY5VHZRX","colab_type":"code","outputId":"e1e781dc-1748-4e63-dde0-e5786433549b","executionInfo":{"status":"ok","timestamp":1573880944431,"user_tz":-330,"elapsed":4864,"user":{"displayName":"Srihari Vemuru","photoUrl":"","userId":"13401425132614860256"}},"colab":{"base_uri":"https://localhost:8080/","height":364}},"source":["for i in num_non_cat:\n","    print(train[i].isnull().sum())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bcUkEvyMHZRi","colab_type":"code","colab":{}},"source":["columns = train.columns\n","num_cols = []\n","num_non_cat = []\n","for i in columns:\n","    if(str(train[i].dtype) == 'category'):\n","        pass\n","    else:\n","        num_cols.append(i)\n","for i in num_cols:\n","    if(i not in num_cat):\n","        num_non_cat.append(i)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KOPp2AE6HZRr","colab_type":"text"},"source":["### Categorical Encoding###"]},{"cell_type":"code","metadata":{"id":"MMHT9lEHHZRs","colab_type":"code","colab":{}},"source":["train['SmartScreen'].replace(\n","    {'Prompt': 'prompt', \n","    'warn': 'prompt',\n","    'Block': 'on',\n","    'On': 'on',\n","    'Off': 'off',\n","    'ExistsNotSet': 'off'},\n","     inplace=True)\n","\n","for i in cat_cols:\n","    train[i] = train[i].str.lower()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qgKKP4hFVVu3","colab_type":"code","outputId":"8d2f89ff-ece1-4e36-dcf3-ba1deafa4871","executionInfo":{"status":"ok","timestamp":1573880944434,"user_tz":-330,"elapsed":4845,"user":{"displayName":"Srihari Vemuru","photoUrl":"","userId":"13401425132614860256"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cat_l10 = []\n","j = 0\n","for i in num_cat:\n","  if(len(train[i].unique()) <= 10):\n","    cat_l10.append(i)\n","    num_cat.pop(j)\n","  j += 1\n","\n","j=0\n","for i in cat_cols:\n","  if(len(train[i].unique()) <= 10):\n","    cat_l10.append(i)\n","    cat_cols.pop(j)\n","  j += 1\n","len(cat_l10)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["15"]},"metadata":{"tags":[]},"execution_count":80}]},{"cell_type":"code","metadata":{"id":"HkCHzV6vacp_","colab_type":"code","colab":{}},"source":["def create_dummies(df, column_name):\n","    dummies = pd.get_dummies(df[column_name], prefix=column_name)\n","    df = df.drop(columns = column_name)\n","    df = pd.concat([df, dummies], axis=1)\n","    return df\n","\n","for i in cat_l10:\n","  train = create_dummies(train, i)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"04oP4giEHZR3","colab_type":"code","outputId":"3a1997af-35e4-43cc-b653-1bdd1500e587","executionInfo":{"status":"ok","timestamp":1573880960011,"user_tz":-330,"elapsed":20406,"user":{"displayName":"Srihari Vemuru","photoUrl":"","userId":"13401425132614860256"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["def g(d, variable):\n","    freq_dict = {} \n","\n","    for i in range(len(d[variable])):\n","        \n","#         print(d[variable][i])\n","        if not pd.isnull(d[variable][i]):\n","            if d[variable][i] in freq_dict:\n","                freq_dict[d[variable][i]] += 1\n","            else:\n","                freq_dict.update({d[variable][i]: 1})\n","    \n","    for i in freq_dict:\n","        freq_dict.update({i:freq_dict[i]/d[variable].count()})\n","     \n","\n","        \n","    return freq_dict\n","\n","# def frequency_encoding(variable):\n","#     t = train[variable].value_counts().reset_index()\n","#     t = t.reset_index()\n","#     t.loc[t[variable] == 1, 'level_0'] = np.nan\n","#     t.set_index('index', inplace=True)\n","#     max_label = t['level_0'].max() + 1\n","#     t.fillna(max_label, inplace=True)\n","#     return t.to_dict()['level_0']\n","\n","for col in tqdm(num_cat):\n","    freq_enc_dict = g(train, col)\n","    train[col].replace(freq_enc_dict,inplace = True)\n","    \n","for col in tqdm(cat_cols):\n","    freq_enc_dict = g(train, col)\n","    train[col].replace(freq_enc_dict,inplace = True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 17/17 [00:07<00:00,  2.70it/s]\n","100%|██████████| 21/21 [00:07<00:00,  1.77it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"uq_xc8U_dKvx","colab_type":"code","colab":{}},"source":["columns = train.columns\n","num_cols = []\n","num_non_cat = []\n","for i in columns:\n","    if(str(train[i].dtype) == 'category'):\n","        pass\n","    else:\n","        num_cols.append(i)\n","for i in num_cols:\n","    if(i not in num_cat):\n","        num_non_cat.append(i)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dxxLla69HZR_","colab_type":"text"},"source":["### Categorical Imputing##"]},{"cell_type":"markdown","metadata":{"id":"XAqKhprYHZSE","colab_type":"text"},"source":["**Num**"]},{"cell_type":"code","metadata":{"id":"urR3v_JuHZSG","colab_type":"code","colab":{}},"source":["for i in num_cat:\n","    train[i] = train[i].fillna(0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"HSwNd78tHZSP","colab_type":"code","outputId":"4a1bc808-1cab-4c7b-f92a-96ebf55e98b2","executionInfo":{"status":"ok","timestamp":1573880960820,"user_tz":-330,"elapsed":21196,"user":{"displayName":"Srihari Vemuru","photoUrl":"","userId":"13401425132614860256"}},"colab":{"base_uri":"https://localhost:8080/","height":312}},"source":["for i in num_cat:\n","    print(train[i].isnull().sum())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"l1O5Nsb0HZSY","colab_type":"text"},"source":["**Non-Num**"]},{"cell_type":"code","metadata":{"id":"F18MhU04HZSb","colab_type":"code","colab":{}},"source":["for i in cat_cols:\n","    train[i] = train[i].fillna(0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"wb9PMBrKHZSm","colab_type":"code","outputId":"581db7cc-45dd-43b9-df55-df89911af63f","executionInfo":{"status":"ok","timestamp":1573880960822,"user_tz":-330,"elapsed":21180,"user":{"displayName":"Srihari Vemuru","photoUrl":"","userId":"13401425132614860256"}},"colab":{"base_uri":"https://localhost:8080/","height":382}},"source":["for i in cat_cols:\n","    print(train[i].isnull().sum())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OeA2dooWHZSt","colab_type":"text"},"source":["### Adding Extra Cols"]},{"cell_type":"code","metadata":{"id":"baUoE5qFHZSw","colab_type":"code","colab":{}},"source":["# total disk capacity remaining\n","train['disk_remain'] = train['Census_PrimaryDiskTotalCapacity'] - train['Census_SystemVolumeTotalCapacity']\n","train['disk_remain'] = train['disk_remain'].astype('float32')\n","\n","# Ram-to-CPU ratio\n","train['ram_cpu_ratio'] = train['Census_TotalPhysicalRAM'] / train['Census_ProcessorCoreCount']\n","\n","# Pixel Per Inch PPI sqrt(horizonal**2 + vertical**2) / diagonal\n","train['ppi'] = np.sqrt(train['Census_InternalPrimaryDisplayResolutionHorizontal']**2 + train['Census_InternalPrimaryDisplayResolutionVertical']**2) / train['Census_InternalPrimaryDiagonalDisplaySizeInInches']\n","\n","# PPI squared\n","train['ppi2'] = train.ppi ** 2\n","\n","# Screen aspect ratio = Horizonal / Vertical\n","train['aspect_ratio'] = train['Census_InternalPrimaryDisplayResolutionHorizontal'] / train['Census_InternalPrimaryDisplayResolutionVertical']\n","\n","# Pixel count = Horizonal * Vertical\n","train['pixel_count'] = train['Census_InternalPrimaryDisplayResolutionHorizontal'] * train['Census_InternalPrimaryDisplayResolutionVertical']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RjwHnEY4HZS3","colab_type":"code","outputId":"c5799438-bce3-4c35-b0f7-39fc36381fca","executionInfo":{"status":"ok","timestamp":1573880960824,"user_tz":-330,"elapsed":21168,"user":{"displayName":"Srihari Vemuru","photoUrl":"","userId":"13401425132614860256"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["train.isnull().sum().sum()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":89}]},{"cell_type":"code","metadata":{"id":"_GHmhWYvHZS9","colab_type":"code","colab":{}},"source":["train.to_csv('/content/gdrive/My Drive/ML_datasets/train_rev.csv', index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xdmaj2joHZTN","colab_type":"text"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"aURT8BGIHZTO","colab_type":"code","colab":{}},"source":["train = pd.read_csv('/content/gdrive/My Drive/ML_datasets/train_rev.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vr2QUSVJlHdh","colab_type":"code","colab":{}},"source":["for i in train.columns:\n","  ma = train[i].max()\n","  mi = train[i].min()\n","  train[i] = (train[i]-mi)/(ma-mi)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CfuHEVH_lWB9","colab_type":"code","outputId":"acaaabd2-e407-4bbe-e3ec-9579ae92a57e","executionInfo":{"status":"ok","timestamp":1574100724325,"user_tz":-330,"elapsed":713,"user":{"displayName":"Srihari Vemuru","photoUrl":"","userId":"13401425132614860256"}},"colab":{"base_uri":"https://localhost:8080/","height":255}},"source":["train.head()"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>IsSxsPassiveMode</th>\n","      <th>AVProductStatesIdentifier</th>\n","      <th>AVProductsInstalled</th>\n","      <th>AVProductsEnabled</th>\n","      <th>HasTpm</th>\n","      <th>CountryIdentifier</th>\n","      <th>CityIdentifier</th>\n","      <th>OrganizationIdentifier</th>\n","      <th>GeoNameIdentifier</th>\n","      <th>LocaleEnglishNameIdentifier</th>\n","      <th>Platform</th>\n","      <th>OsVer</th>\n","      <th>OsBuild</th>\n","      <th>SkuEdition</th>\n","      <th>IsProtected</th>\n","      <th>IeVerIdentifier</th>\n","      <th>Firewall</th>\n","      <th>Census_MDC2FormFactor</th>\n","      <th>Census_OEMNameIdentifier</th>\n","      <th>Census_OEMModelIdentifier</th>\n","      <th>Census_ProcessorCoreCount</th>\n","      <th>Census_ProcessorModelIdentifier</th>\n","      <th>Census_PrimaryDiskTotalCapacity</th>\n","      <th>Census_PrimaryDiskTypeName</th>\n","      <th>Census_SystemVolumeTotalCapacity</th>\n","      <th>Census_HasOpticalDiskDrive</th>\n","      <th>Census_TotalPhysicalRAM</th>\n","      <th>Census_ChassisTypeName</th>\n","      <th>Census_InternalPrimaryDiagonalDisplaySizeInInches</th>\n","      <th>Census_InternalPrimaryDisplayResolutionHorizontal</th>\n","      <th>Census_InternalPrimaryDisplayResolutionVertical</th>\n","      <th>Census_InternalBatteryType</th>\n","      <th>Census_InternalBatteryNumberOfCharges</th>\n","      <th>Census_OSVersion</th>\n","      <th>Census_OSBranch</th>\n","      <th>Census_OSBuildNumber</th>\n","      <th>Census_OSBuildRevision</th>\n","      <th>Census_OSEdition</th>\n","      <th>Census_OSSkuName</th>\n","      <th>Census_OSInstallLanguageIdentifier</th>\n","      <th>...</th>\n","      <th>Census_PowerPlatformRoleName_slate</th>\n","      <th>Census_PowerPlatformRoleName_sohoserver</th>\n","      <th>Census_PowerPlatformRoleName_unknown</th>\n","      <th>Census_PowerPlatformRoleName_workstation</th>\n","      <th>Census_OSArchitecture_amd64</th>\n","      <th>Census_OSArchitecture_arm64</th>\n","      <th>Census_OSArchitecture_x86</th>\n","      <th>Census_OSInstallTypeName_clean</th>\n","      <th>Census_OSInstallTypeName_cleanpcrefresh</th>\n","      <th>Census_OSInstallTypeName_ibsclean</th>\n","      <th>Census_OSInstallTypeName_other</th>\n","      <th>Census_OSInstallTypeName_refresh</th>\n","      <th>Census_OSInstallTypeName_reset</th>\n","      <th>Census_OSInstallTypeName_update</th>\n","      <th>Census_OSInstallTypeName_upgrade</th>\n","      <th>Census_OSInstallTypeName_uupupgrade</th>\n","      <th>Census_GenuineStateName_invalid_license</th>\n","      <th>Census_GenuineStateName_is_genuine</th>\n","      <th>Census_GenuineStateName_offline</th>\n","      <th>Census_GenuineStateName_unknown</th>\n","      <th>Census_FlightRing_disabled</th>\n","      <th>Census_FlightRing_not_set</th>\n","      <th>Census_FlightRing_retail</th>\n","      <th>Census_FlightRing_rp</th>\n","      <th>Census_FlightRing_unknown</th>\n","      <th>Census_FlightRing_wif</th>\n","      <th>Census_FlightRing_wis</th>\n","      <th>EngineVersion4_0</th>\n","      <th>EngineVersion4_1</th>\n","      <th>EngineVersion4_2</th>\n","      <th>EngineVersion4_3</th>\n","      <th>EngineVersion4_4</th>\n","      <th>EngineVersion4_5</th>\n","      <th>EngineVersion4_6</th>\n","      <th>disk_remain</th>\n","      <th>ram_cpu_ratio</th>\n","      <th>ppi</th>\n","      <th>ppi2</th>\n","      <th>aspect_ratio</th>\n","      <th>pixel_count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>0.012612</td>\n","      <td>0.000000</td>\n","      <td>0.333333</td>\n","      <td>0.0</td>\n","      <td>0.180645</td>\n","      <td>0.008264</td>\n","      <td>1.000000</td>\n","      <td>0.028201</td>\n","      <td>0.643636</td>\n","      <td>0.009446</td>\n","      <td>0.011000</td>\n","      <td>0.024031</td>\n","      <td>0.613658</td>\n","      <td>1.0</td>\n","      <td>0.006114</td>\n","      <td>1.0</td>\n","      <td>0.351038</td>\n","      <td>0.774503</td>\n","      <td>0.015584</td>\n","      <td>0.025641</td>\n","      <td>0.072289</td>\n","      <td>0.036254</td>\n","      <td>0.049881</td>\n","      <td>0.048352</td>\n","      <td>0.0</td>\n","      <td>0.023622</td>\n","      <td>0.373050</td>\n","      <td>0.184555</td>\n","      <td>0.125000</td>\n","      <td>0.226667</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.003672</td>\n","      <td>0.085133</td>\n","      <td>0.150572</td>\n","      <td>0.003672</td>\n","      <td>0.947817</td>\n","      <td>0.964626</td>\n","      <td>0.327918</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.001274</td>\n","      <td>0.230774</td>\n","      <td>0.202117</td>\n","      <td>0.056092</td>\n","      <td>0.144154</td>\n","      <td>0.069504</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.333333</td>\n","      <td>1.0</td>\n","      <td>0.473118</td>\n","      <td>0.115702</td>\n","      <td>0.428977</td>\n","      <td>0.124083</td>\n","      <td>0.160000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.630223</td>\n","      <td>0.613658</td>\n","      <td>1.0</td>\n","      <td>0.451310</td>\n","      <td>1.0</td>\n","      <td>0.351038</td>\n","      <td>0.021933</td>\n","      <td>0.002597</td>\n","      <td>0.076923</td>\n","      <td>0.018072</td>\n","      <td>0.184338</td>\n","      <td>1.000000</td>\n","      <td>0.121877</td>\n","      <td>0.0</td>\n","      <td>0.023622</td>\n","      <td>0.373050</td>\n","      <td>0.236911</td>\n","      <td>0.250000</td>\n","      <td>0.250000</td>\n","      <td>0.000000</td>\n","      <td>1.0</td>\n","      <td>0.015912</td>\n","      <td>0.297967</td>\n","      <td>0.603770</td>\n","      <td>0.015912</td>\n","      <td>0.947817</td>\n","      <td>0.964626</td>\n","      <td>0.073924</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.100414</td>\n","      <td>0.102570</td>\n","      <td>0.230134</td>\n","      <td>0.069705</td>\n","      <td>0.254774</td>\n","      <td>0.122340</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>0.000145</td>\n","      <td>0.333333</td>\n","      <td>0.666667</td>\n","      <td>1.0</td>\n","      <td>0.546237</td>\n","      <td>0.140496</td>\n","      <td>1.000000</td>\n","      <td>0.138184</td>\n","      <td>0.905455</td>\n","      <td>0.019887</td>\n","      <td>0.021504</td>\n","      <td>0.046980</td>\n","      <td>1.000000</td>\n","      <td>1.0</td>\n","      <td>0.039083</td>\n","      <td>1.0</td>\n","      <td>1.000000</td>\n","      <td>0.774503</td>\n","      <td>0.020779</td>\n","      <td>0.076923</td>\n","      <td>0.162651</td>\n","      <td>0.247081</td>\n","      <td>1.000000</td>\n","      <td>0.326720</td>\n","      <td>0.0</td>\n","      <td>0.055118</td>\n","      <td>0.069366</td>\n","      <td>0.138743</td>\n","      <td>0.141797</td>\n","      <td>0.120000</td>\n","      <td>0.012511</td>\n","      <td>0.0</td>\n","      <td>0.018360</td>\n","      <td>0.085133</td>\n","      <td>0.150572</td>\n","      <td>0.018360</td>\n","      <td>0.561214</td>\n","      <td>0.561214</td>\n","      <td>0.062551</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.003495</td>\n","      <td>0.230774</td>\n","      <td>0.245827</td>\n","      <td>0.077952</td>\n","      <td>0.254956</td>\n","      <td>0.051383</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.333333</td>\n","      <td>1.0</td>\n","      <td>0.038710</td>\n","      <td>0.132231</td>\n","      <td>0.000000</td>\n","      <td>0.005076</td>\n","      <td>0.960000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.613658</td>\n","      <td>1.0</td>\n","      <td>1.000000</td>\n","      <td>1.0</td>\n","      <td>1.000000</td>\n","      <td>0.774503</td>\n","      <td>0.012987</td>\n","      <td>0.076923</td>\n","      <td>0.292169</td>\n","      <td>0.121595</td>\n","      <td>1.000000</td>\n","      <td>0.163355</td>\n","      <td>0.0</td>\n","      <td>0.023622</td>\n","      <td>1.000000</td>\n","      <td>0.117801</td>\n","      <td>0.141797</td>\n","      <td>0.120000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.947817</td>\n","      <td>0.964626</td>\n","      <td>1.000000</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000316</td>\n","      <td>0.102570</td>\n","      <td>0.280131</td>\n","      <td>0.097531</td>\n","      <td>0.254956</td>\n","      <td>0.051383</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.0</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.333333</td>\n","      <td>1.0</td>\n","      <td>0.481720</td>\n","      <td>0.545455</td>\n","      <td>1.000000</td>\n","      <td>0.040609</td>\n","      <td>0.265455</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.613658</td>\n","      <td>1.0</td>\n","      <td>1.000000</td>\n","      <td>1.0</td>\n","      <td>1.000000</td>\n","      <td>0.056888</td>\n","      <td>0.002597</td>\n","      <td>0.179487</td>\n","      <td>0.259036</td>\n","      <td>0.058852</td>\n","      <td>0.439964</td>\n","      <td>0.034467</td>\n","      <td>0.0</td>\n","      <td>0.118110</td>\n","      <td>1.000000</td>\n","      <td>0.138743</td>\n","      <td>0.250000</td>\n","      <td>0.250000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.079559</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.086291</td>\n","      <td>0.947817</td>\n","      <td>0.964626</td>\n","      <td>1.000000</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.036483</td>\n","      <td>0.230774</td>\n","      <td>0.366739</td>\n","      <td>0.156446</td>\n","      <td>0.254774</td>\n","      <td>0.122340</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 147 columns</p>\n","</div>"],"text/plain":["   IsSxsPassiveMode  AVProductStatesIdentifier  ...  aspect_ratio  pixel_count\n","0               0.0                   0.012612  ...      0.144154     0.069504\n","1               0.0                   1.000000  ...      0.254774     0.122340\n","2               0.0                   0.000145  ...      0.254956     0.051383\n","3               0.0                   1.000000  ...      0.254956     0.051383\n","4               0.0                   1.000000  ...      0.254774     0.122340\n","\n","[5 rows x 147 columns]"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"0MFxqaTEHZTY","colab_type":"code","colab":{}},"source":["# X_train = train\n","# y_train = target"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"faaRTJNNsyH3","colab_type":"code","colab":{}},"source":["X_train, X_val, y_train, y_val = train_test_split(train, target, test_size = 0.2, random_state =49)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gZ8vTrADHZTi","colab_type":"code","outputId":"a70da6d8-b314-4f49-88f2-83369986ca11","executionInfo":{"status":"ok","timestamp":1574100730161,"user_tz":-330,"elapsed":1249,"user":{"displayName":"Srihari Vemuru","photoUrl":"","userId":"13401425132614860256"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["X_train.shape"],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8339, 147)"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"ECmNkUjmHZTn","colab_type":"code","colab":{}},"source":["# # fast & efficient way to calculate mean using numba\n","# @jit(parallel=True)\n","# def mean_score(array):\n","#     return np.mean(array)\n","\n","# # fast & efficient way to calculate division using numba\n","# @jit(parallel=True)\n","# def jit_div(array, divisor):\n","#     return np.divide(array, divisor)\n","\n","# def run_lgb(train, target, test):\n","#     global clf\n","    \n","#     kf = KFold(n_splits=3, shuffle=True)\n","\n","#     train_scores = []\n","#     cv_scores = []\n","#     train_predictions = np.zeros(train.shape[0], dtype='float32')\n","#     test_predictions = np.zeros(test.shape[0], dtype='float32')\n","    \n","#     for (train_index, test_index) in tqdm(kf.split(train, target)):\n","#         x_train, y_train = train.iloc[train_index], target.iloc[train_index]\n","#         x_test, y_test = train.iloc[test_index], target.iloc[test_index]\n","\n","#         clf = lgb.LGBMClassifier(boosting_type='gbdt', \n","#                                  num_leaves=250, \n","#                                  n_estimators=2000, \n","#                                  learning_rate=0.02)\n","\n","#         clf.fit(x_train, y_train, eval_metric='auc',  \n","#                 eval_set=[(x_test, y_test)], \n","#                 verbose=200,\n","#                 early_stopping_rounds=100)\n","#         pred = clf.predict(x_train)\n","#         score = roc_auc_score(y_train, pred)\n","#         train_scores.append(score)\n","\n","#         pred = clf.predict(x_test)\n","#         score = roc_auc_score(y_test, pred)\n","#         cv_scores.append(score)\n","        \n","#         # predict probabilities for train\n","#         pred_prob = clf.predict_proba(train)[:,1]\n","#         train_predictions = np.add(train_predictions, pred_prob).astype('float32')\n","        \n","#         # predict probabilities for test\n","#         pred_prob = clf.predict_proba(test)[:,1]\n","#         test_predictions = np.add(test_predictions, pred_prob).astype('float32')\n","    \n","#     print(f'training sklearn auc: {mean_score(train_scores):.4f}')\n","#     print(f'3-fold CV sklearn auc: {mean_score(cv_scores):.4f}')\n","\n","#     return jit_div(train_predictions, 3), jit_div(test_predictions, 3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lj1aYVthHZTw","colab_type":"code","colab":{}},"source":["# clf = lgb.LGBMClassifier(boosting_type='gbdt', \n","#                                  num_leaves=250, \n","#                                  n_estimators=2000, \n","#                                  learning_rate=0.02)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YIpCA5CIzyDK","colab_type":"code","colab":{}},"source":["# clf = SVC()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7AT5_SemIxJK","colab_type":"code","colab":{}},"source":["# clf.fit(X_train, y_train) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RLWHrEhsJGXk","colab_type":"code","outputId":"d148abc4-c44a-40c5-e63a-edae8c6f6870","executionInfo":{"status":"ok","timestamp":1573880757611,"user_tz":-330,"elapsed":1271,"user":{"displayName":"Srihari Vemuru","photoUrl":"","userId":"13401425132614860256"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# 'HasDetections' in X_train.columns"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"ViozK14ZJdrR","colab_type":"code","colab":{}},"source":["# pred = clf.predict(X_val)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_tTDF1rIHZT3","colab_type":"code","outputId":"807d6b91-45b5-441e-ef9e-6685af2da5ef","executionInfo":{"status":"ok","timestamp":1573834058546,"user_tz":-330,"elapsed":59780,"user":{"displayName":"Srihari Vemuru","photoUrl":"","userId":"13401425132614860256"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# score =accuracy_score(y_val,pred)\n","# score"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.49089165867689355"]},"metadata":{"tags":[]},"execution_count":131}]},{"cell_type":"markdown","metadata":{"id":"X0MUDfikfeVL","colab_type":"text"},"source":["**NeuralNet**"]},{"cell_type":"code","metadata":{"id":"ecnoiqI7fkec","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","torch.set_printoptions(linewidth=120)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P7gyUAkzqr5Q","colab_type":"code","colab":{}},"source":["class SystemData(object):\n","  def __init__(self, data, labels):\n","    self.labels = labels\n","    self.data = data\n","  \n","  def __getitem__(self, index):\n","    d = torch.tensor(self.data[index])\n","    l = self.labels[index]\n","    return d, l\n","  \n","  def __len__(self):\n","    return len(self.labels)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GHPb9OIqILY5","colab_type":"code","outputId":"73a6e256-1b4a-4dbb-b792-b8b2b25c5927","executionInfo":{"status":"ok","timestamp":1574100751891,"user_tz":-330,"elapsed":1339,"user":{"displayName":"Srihari Vemuru","photoUrl":"","userId":"13401425132614860256"}},"colab":{"base_uri":"https://localhost:8080/","height":225}},"source":["y_train"],"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4742    0\n","8392    1\n","2       1\n","7808    0\n","8796    0\n","       ..\n","8050    1\n","7924    1\n","3254    0\n","5805    1\n","426     1\n","Name: HasDetections, Length: 8339, dtype: int64"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"id":"dOoFk7Nqo8aU","colab_type":"code","colab":{}},"source":["dset = SystemData(X_train.values, y_train.values)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_qw6ng1bp6mo","colab_type":"code","colab":{}},"source":["dloader = torch.utils.data.DataLoader(dset, batch_size=59)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nw9URhplcxA-","colab_type":"code","colab":{}},"source":["batches = iter(dloader)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2k2rjE5Vdbm6","colab_type":"code","colab":{}},"source":["class Network(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.fc1 = nn.Linear(in_features=147, out_features=200)\n","    self.fc2 = nn.Linear(in_features=200, out_features=60)\n","\n","    self.fc3 = nn.Linear(in_features=60, out_features=30)\n","    \n","    self.fc4 = nn.Linear(in_features=30, out_features=30)\n","    # self.fc5 = nn.Linear(in_features=30, out_features=30)\n","    # self.fc6 = nn.Linear(in_features=30, out_features=30)\n","    # self.fc7 = nn.Linear(in_features=30, out_features=30)\n","    # self.fc8 = nn.Linear(in_features=30, out_features=30)\n","    # self.fc9 = nn.Linear(in_features=30, out_features=30)\n","    # self.fc10 = nn.Linear(in_features=30, out_features=30)\n","    # self.fc11 = nn.Linear(in_features=30, out_features=30)\n","    # self.fc12 = nn.Linear(in_features=30, out_features=30)\n","    # self.fc13 = nn.Linear(in_features=30, out_features=30)\n","\n","\n","    self.fc14 = nn. Linear(in_features=30, out_features=10)\n","    self.out = nn. Linear(in_features=10, out_features=2)\n","\n","  def forward(self, t):\n","    t = self.fc1(t)\n","    t = F.relu(t)\n","\n","    t = self.fc2(t)\n","    t = F.relu(t)\n","\n","    t = self.fc3(t)\n","    t = F.relu(t)\n","\n","    t = self.fc4(t)\n","    t = F.relu(t)\n","\n","    # t = self.fc5(t)\n","    # t = F.relu(t)\n","\n","    # t = self.fc6(t)\n","    # t = F.relu(t)\n","\n","    # t = self.fc7(t)\n","    # t = F.relu(t)\n","\n","    # t = self.fc8(t)\n","    # t = F.relu(t)\n","\n","    # t = self.fc9(t)\n","    # t = F.relu(t)\n","    # t = self.fc5(t)\n","    # t = F.relu(t)\n","\n","    # t = self.fc10(t)\n","    # t = F.relu(t)\n","\n","    # t = self.fc11(t)\n","    # t = F.relu(t)\n","\n","    # t = self.fc12(t)\n","    # t = F.relu(t)\n","\n","    # t = self.fc13(t)\n","    # t = F.relu(t)\n","\n","    t = self.fc14(t)\n","    t = F.relu(t)\n","    t = self.out(t)\n","\n","    t = F.softmax(t, dim=1)\n","    return t"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JHJMQtzXe17l","colab_type":"code","colab":{}},"source":["network = Network()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HIsojcGRfA2Y","colab_type":"code","colab":{}},"source":["def get_num_correct(preds, labels):\n","    return (preds.argmax(dim=1)==labels).sum().item()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kr2sSLw_fK2s","colab_type":"code","colab":{}},"source":["# p = network(data.float())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yqnxf692Dfky","colab_type":"code","colab":{}},"source":["dset1 = SystemData(X_val.values, y_val.values)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VPbM5nUJNyO3","colab_type":"code","colab":{}},"source":["dloader1 = torch.utils.data.DataLoader(dset1, batch_size=len(y_val.values))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5vUA7eTxOcLW","colab_type":"code","colab":{}},"source":["batches1 = iter(dloader1)\n","data1, labels1 = next(batches1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"43-R_Vv6OnFw","colab_type":"code","outputId":"6c3d3c9c-d771-46c8-cfa2-e1053df8c252","executionInfo":{"status":"ok","timestamp":1574102299620,"user_tz":-330,"elapsed":622,"user":{"displayName":"Srihari Vemuru","photoUrl":"","userId":"13401425132614860256"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["data1.shape"],"execution_count":180,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2085, 147])"]},"metadata":{"tags":[]},"execution_count":180}]},{"cell_type":"code","metadata":{"id":"cB7JzWBrAJ2C","colab_type":"code","outputId":"b4e68c19-ce90-4b3a-9570-1a3c7a632dce","executionInfo":{"status":"ok","timestamp":1574092738309,"user_tz":-330,"elapsed":48743,"user":{"displayName":"Srihari Vemuru","photoUrl":"","userId":"13401425132614860256"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["optimizer = optim.Adam(network.parameters(), lr=0.00001)\n","\n","for epoch in range(9999):\n","  total_loss = 0\n","  total_correct = 0\n","  for batch in dloader:\n","    data2, labels = batch\n","\n","    predictions = network(data2.float())\n","    loss = F.cross_entropy(predictions, labels)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    total_loss += loss.item()\n","    total_correct += get_num_correct(predictions, labels)\n","\n","  pred1 = network(data1.float())\n","  ans=[]\n","  for t in pred1:\n","    x, y = t\n","    if (x>0.4):\n","      ans.append(0)\n","    else:\n","      ans.append(1)\n","  score =accuracy_score(y_val,ans)\n","\n","  print('epoch: ', epoch, 'totalCorrect: ', total_correct, 'loss: ', total_loss, 'Val_score', score )\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["epoch:  0 totalCorrect:  5122 loss:  94.98689490556717 Val_score 0.5760191846522782\n","epoch:  1 totalCorrect:  5124 loss:  94.88247871398926 Val_score 0.5774580335731415\n","epoch:  2 totalCorrect:  5120 loss:  94.78006970882416 Val_score 0.5808153477218225\n","epoch:  3 totalCorrect:  5122 loss:  94.67792111635208 Val_score 0.5827338129496403\n","epoch:  4 totalCorrect:  5119 loss:  94.57648760080338 Val_score 0.5880095923261391\n","epoch:  5 totalCorrect:  5119 loss:  94.476164996624 Val_score 0.592326139088729\n","epoch:  6 totalCorrect:  5124 loss:  94.37696868181229 Val_score 0.5932853717026378\n","epoch:  7 totalCorrect:  5133 loss:  94.27901166677475 Val_score 0.592326139088729\n","epoch:  8 totalCorrect:  5137 loss:  94.18346440792084 Val_score 0.5937649880095923\n","epoch:  9 totalCorrect:  5138 loss:  94.08997136354446 Val_score 0.5961630695443645\n","epoch:  10 totalCorrect:  5145 loss:  93.9989664554596 Val_score 0.5980815347721823\n","epoch:  11 totalCorrect:  5158 loss:  93.9107141494751 Val_score 0.596642685851319\n","epoch:  12 totalCorrect:  5160 loss:  93.82477676868439 Val_score 0.5952038369304556\n","epoch:  13 totalCorrect:  5161 loss:  93.74176400899887 Val_score 0.5961630695443645\n","epoch:  14 totalCorrect:  5164 loss:  93.66174077987671 Val_score 0.5952038369304556\n","epoch:  15 totalCorrect:  5161 loss:  93.58445191383362 Val_score 0.5932853717026378\n","epoch:  16 totalCorrect:  5158 loss:  93.50955104827881 Val_score 0.5952038369304556\n","epoch:  17 totalCorrect:  5162 loss:  93.43666964769363 Val_score 0.5932853717026378\n","epoch:  18 totalCorrect:  5169 loss:  93.36631619930267 Val_score 0.5956834532374101\n","epoch:  19 totalCorrect:  5167 loss:  93.29870319366455 Val_score 0.5952038369304556\n","epoch:  20 totalCorrect:  5167 loss:  93.2330567240715 Val_score 0.5937649880095923\n","epoch:  21 totalCorrect:  5165 loss:  93.16989266872406 Val_score 0.5942446043165468\n","epoch:  22 totalCorrect:  5163 loss:  93.10880452394485 Val_score 0.596642685851319\n","epoch:  23 totalCorrect:  5164 loss:  93.05005699396133 Val_score 0.5976019184652278\n","epoch:  24 totalCorrect:  5162 loss:  92.99364107847214 Val_score 0.5971223021582733\n","epoch:  25 totalCorrect:  5160 loss:  92.93902432918549 Val_score 0.5971223021582733\n","epoch:  26 totalCorrect:  5164 loss:  92.88658797740936 Val_score 0.5956834532374101\n","epoch:  27 totalCorrect:  5156 loss:  92.83613234758377 Val_score 0.5947242206235012\n","epoch:  28 totalCorrect:  5160 loss:  92.78752171993256 Val_score 0.5932853717026378\n","epoch:  29 totalCorrect:  5163 loss:  92.74072027206421 Val_score 0.5937649880095923\n","epoch:  30 totalCorrect:  5164 loss:  92.69564324617386 Val_score 0.5913669064748202\n","epoch:  31 totalCorrect:  5165 loss:  92.65215218067169 Val_score 0.5918465227817746\n","epoch:  32 totalCorrect:  5175 loss:  92.61022734642029 Val_score 0.5908872901678657\n","epoch:  33 totalCorrect:  5182 loss:  92.56977611780167 Val_score 0.5913669064748202\n","epoch:  34 totalCorrect:  5184 loss:  92.53099232912064 Val_score 0.5918465227817746\n","epoch:  35 totalCorrect:  5185 loss:  92.49336379766464 Val_score 0.5918465227817746\n","epoch:  36 totalCorrect:  5187 loss:  92.45702761411667 Val_score 0.592326139088729\n","epoch:  37 totalCorrect:  5195 loss:  92.42237311601639 Val_score 0.5937649880095923\n","epoch:  38 totalCorrect:  5193 loss:  92.38858169317245 Val_score 0.5932853717026378\n","epoch:  39 totalCorrect:  5194 loss:  92.35623490810394 Val_score 0.5928057553956835\n","epoch:  40 totalCorrect:  5195 loss:  92.32464855909348 Val_score 0.5937649880095923\n","epoch:  41 totalCorrect:  5194 loss:  92.29368686676025 Val_score 0.5932853717026378\n","epoch:  42 totalCorrect:  5197 loss:  92.26389807462692 Val_score 0.5937649880095923\n","epoch:  43 totalCorrect:  5198 loss:  92.23488658666611 Val_score 0.5932853717026378\n","epoch:  44 totalCorrect:  5197 loss:  92.20679342746735 Val_score 0.5928057553956835\n","epoch:  45 totalCorrect:  5199 loss:  92.17951053380966 Val_score 0.5928057553956835\n","epoch:  46 totalCorrect:  5195 loss:  92.15304177999496 Val_score 0.5928057553956835\n","epoch:  47 totalCorrect:  5200 loss:  92.12725347280502 Val_score 0.592326139088729\n","epoch:  48 totalCorrect:  5207 loss:  92.10226345062256 Val_score 0.5932853717026378\n","epoch:  49 totalCorrect:  5208 loss:  92.07803839445114 Val_score 0.592326139088729\n","epoch:  50 totalCorrect:  5209 loss:  92.0543891787529 Val_score 0.5928057553956835\n","epoch:  51 totalCorrect:  5211 loss:  92.03117853403091 Val_score 0.5918465227817746\n","epoch:  52 totalCorrect:  5212 loss:  92.0085659623146 Val_score 0.5904076738609113\n","epoch:  53 totalCorrect:  5214 loss:  91.98654866218567 Val_score 0.5908872901678657\n","epoch:  54 totalCorrect:  5219 loss:  91.9651871919632 Val_score 0.5913669064748202\n","epoch:  55 totalCorrect:  5223 loss:  91.94433611631393 Val_score 0.5913669064748202\n","epoch:  56 totalCorrect:  5228 loss:  91.92394959926605 Val_score 0.5908872901678657\n","epoch:  57 totalCorrect:  5229 loss:  91.90389078855515 Val_score 0.5899280575539568\n","epoch:  58 totalCorrect:  5229 loss:  91.8845881819725 Val_score 0.5899280575539568\n","epoch:  59 totalCorrect:  5231 loss:  91.8653370141983 Val_score 0.5899280575539568\n","epoch:  60 totalCorrect:  5237 loss:  91.84660559892654 Val_score 0.5894484412470024\n","epoch:  61 totalCorrect:  5239 loss:  91.82818698883057 Val_score 0.5884892086330935\n","epoch:  62 totalCorrect:  5235 loss:  91.81003361940384 Val_score 0.5894484412470024\n","epoch:  63 totalCorrect:  5238 loss:  91.79221552610397 Val_score 0.588968824940048\n","epoch:  64 totalCorrect:  5239 loss:  91.77483719587326 Val_score 0.5880095923261391\n","epoch:  65 totalCorrect:  5238 loss:  91.75757175683975 Val_score 0.588968824940048\n","epoch:  66 totalCorrect:  5240 loss:  91.74063259363174 Val_score 0.5884892086330935\n","epoch:  67 totalCorrect:  5240 loss:  91.7239540219307 Val_score 0.5880095923261391\n","epoch:  68 totalCorrect:  5238 loss:  91.70743542909622 Val_score 0.5884892086330935\n","epoch:  69 totalCorrect:  5241 loss:  91.69133168458939 Val_score 0.5894484412470024\n","epoch:  70 totalCorrect:  5244 loss:  91.67545503377914 Val_score 0.5894484412470024\n","epoch:  71 totalCorrect:  5245 loss:  91.65989059209824 Val_score 0.588968824940048\n","epoch:  72 totalCorrect:  5247 loss:  91.64458072185516 Val_score 0.5894484412470024\n","epoch:  73 totalCorrect:  5241 loss:  91.62939947843552 Val_score 0.5884892086330935\n","epoch:  74 totalCorrect:  5245 loss:  91.6142590045929 Val_score 0.588968824940048\n","epoch:  75 totalCorrect:  5247 loss:  91.5994284749031 Val_score 0.5884892086330935\n","epoch:  76 totalCorrect:  5250 loss:  91.58479046821594 Val_score 0.5884892086330935\n","epoch:  77 totalCorrect:  5248 loss:  91.57023376226425 Val_score 0.5884892086330935\n","epoch:  78 totalCorrect:  5251 loss:  91.55567878484726 Val_score 0.5904076738609113\n","epoch:  79 totalCorrect:  5253 loss:  91.5414006114006 Val_score 0.5904076738609113\n","epoch:  80 totalCorrect:  5250 loss:  91.52720189094543 Val_score 0.5904076738609113\n","epoch:  81 totalCorrect:  5250 loss:  91.51321822404861 Val_score 0.5908872901678657\n","epoch:  82 totalCorrect:  5250 loss:  91.49914574623108 Val_score 0.5913669064748202\n","epoch:  83 totalCorrect:  5256 loss:  91.48520213365555 Val_score 0.5918465227817746\n","epoch:  84 totalCorrect:  5257 loss:  91.47133666276932 Val_score 0.5928057553956835\n","epoch:  85 totalCorrect:  5254 loss:  91.4577619433403 Val_score 0.5937649880095923\n","epoch:  86 totalCorrect:  5257 loss:  91.44432020187378 Val_score 0.5932853717026378\n","epoch:  87 totalCorrect:  5254 loss:  91.43102991580963 Val_score 0.5932853717026378\n","epoch:  88 totalCorrect:  5252 loss:  91.41768652200699 Val_score 0.5937649880095923\n","epoch:  89 totalCorrect:  5253 loss:  91.40468490123749 Val_score 0.5932853717026378\n","epoch:  90 totalCorrect:  5254 loss:  91.39173966646194 Val_score 0.5932853717026378\n","epoch:  91 totalCorrect:  5252 loss:  91.37893790006638 Val_score 0.5928057553956835\n","epoch:  92 totalCorrect:  5252 loss:  91.36622226238251 Val_score 0.5918465227817746\n","epoch:  93 totalCorrect:  5255 loss:  91.35351341962814 Val_score 0.592326139088729\n","epoch:  94 totalCorrect:  5255 loss:  91.34093177318573 Val_score 0.5928057553956835\n","epoch:  95 totalCorrect:  5254 loss:  91.32848483324051 Val_score 0.5928057553956835\n","epoch:  96 totalCorrect:  5258 loss:  91.31602770090103 Val_score 0.5928057553956835\n","epoch:  97 totalCorrect:  5258 loss:  91.30369132757187 Val_score 0.5932853717026378\n","epoch:  98 totalCorrect:  5259 loss:  91.29140758514404 Val_score 0.5937649880095923\n","epoch:  99 totalCorrect:  5256 loss:  91.27922087907791 Val_score 0.5942446043165468\n","epoch:  100 totalCorrect:  5261 loss:  91.26704156398773 Val_score 0.5942446043165468\n","epoch:  101 totalCorrect:  5260 loss:  91.25475496053696 Val_score 0.5937649880095923\n","epoch:  102 totalCorrect:  5261 loss:  91.24221795797348 Val_score 0.5937649880095923\n","epoch:  103 totalCorrect:  5265 loss:  91.23009026050568 Val_score 0.5942446043165468\n","epoch:  104 totalCorrect:  5266 loss:  91.2180410027504 Val_score 0.5942446043165468\n","epoch:  105 totalCorrect:  5272 loss:  91.20595890283585 Val_score 0.5942446043165468\n","epoch:  106 totalCorrect:  5269 loss:  91.19387185573578 Val_score 0.5947242206235012\n","epoch:  107 totalCorrect:  5270 loss:  91.18195217847824 Val_score 0.5937649880095923\n","epoch:  108 totalCorrect:  5270 loss:  91.17015039920807 Val_score 0.5942446043165468\n","epoch:  109 totalCorrect:  5273 loss:  91.1582767367363 Val_score 0.5947242206235012\n","epoch:  110 totalCorrect:  5273 loss:  91.1461974978447 Val_score 0.5947242206235012\n","epoch:  111 totalCorrect:  5276 loss:  91.1340901851654 Val_score 0.5961630695443645\n","epoch:  112 totalCorrect:  5276 loss:  91.1220960021019 Val_score 0.5961630695443645\n","epoch:  113 totalCorrect:  5281 loss:  91.11020785570145 Val_score 0.5961630695443645\n","epoch:  114 totalCorrect:  5281 loss:  91.09831523895264 Val_score 0.5952038369304556\n","epoch:  115 totalCorrect:  5282 loss:  91.08648139238358 Val_score 0.5956834532374101\n","epoch:  116 totalCorrect:  5282 loss:  91.07485550642014 Val_score 0.5961630695443645\n","epoch:  117 totalCorrect:  5279 loss:  91.06306767463684 Val_score 0.5971223021582733\n","epoch:  118 totalCorrect:  5276 loss:  91.05137413740158 Val_score 0.5971223021582733\n","epoch:  119 totalCorrect:  5277 loss:  91.03963381052017 Val_score 0.5976019184652278\n","epoch:  120 totalCorrect:  5275 loss:  91.0279158949852 Val_score 0.5971223021582733\n","epoch:  121 totalCorrect:  5274 loss:  91.0162570476532 Val_score 0.5971223021582733\n","epoch:  122 totalCorrect:  5277 loss:  91.00456953048706 Val_score 0.5980815347721823\n","epoch:  123 totalCorrect:  5279 loss:  90.99296396970749 Val_score 0.5990407673860911\n","epoch:  124 totalCorrect:  5279 loss:  90.98126178979874 Val_score 0.5980815347721823\n","epoch:  125 totalCorrect:  5280 loss:  90.96960425376892 Val_score 0.5990407673860911\n","epoch:  126 totalCorrect:  5283 loss:  90.95791214704514 Val_score 0.5995203836930456\n","epoch:  127 totalCorrect:  5285 loss:  90.94631052017212 Val_score 0.6\n","epoch:  128 totalCorrect:  5290 loss:  90.93456757068634 Val_score 0.6\n","epoch:  129 totalCorrect:  5288 loss:  90.92293608188629 Val_score 0.6\n","epoch:  130 totalCorrect:  5288 loss:  90.91126734018326 Val_score 0.5995203836930456\n","epoch:  131 totalCorrect:  5292 loss:  90.89959615468979 Val_score 0.5995203836930456\n","epoch:  132 totalCorrect:  5295 loss:  90.88793903589249 Val_score 0.6\n","epoch:  133 totalCorrect:  5294 loss:  90.8763918876648 Val_score 0.6\n","epoch:  134 totalCorrect:  5294 loss:  90.86469459533691 Val_score 0.6\n","epoch:  135 totalCorrect:  5293 loss:  90.85305362939835 Val_score 0.5995203836930456\n","epoch:  136 totalCorrect:  5296 loss:  90.84142369031906 Val_score 0.5985611510791367\n","epoch:  137 totalCorrect:  5297 loss:  90.82975614070892 Val_score 0.5985611510791367\n","epoch:  138 totalCorrect:  5299 loss:  90.8181682229042 Val_score 0.5985611510791367\n","epoch:  139 totalCorrect:  5301 loss:  90.80643773078918 Val_score 0.5990407673860911\n","epoch:  140 totalCorrect:  5303 loss:  90.79483115673065 Val_score 0.5995203836930456\n","epoch:  141 totalCorrect:  5307 loss:  90.78314638137817 Val_score 0.5995203836930456\n","epoch:  142 totalCorrect:  5310 loss:  90.7714096903801 Val_score 0.5990407673860911\n","epoch:  143 totalCorrect:  5311 loss:  90.7596110701561 Val_score 0.5995203836930456\n","epoch:  144 totalCorrect:  5310 loss:  90.74779915809631 Val_score 0.5995203836930456\n","epoch:  145 totalCorrect:  5310 loss:  90.73596453666687 Val_score 0.6\n","epoch:  146 totalCorrect:  5309 loss:  90.72411686182022 Val_score 0.5990407673860911\n","epoch:  147 totalCorrect:  5311 loss:  90.71227359771729 Val_score 0.5985611510791367\n","epoch:  148 totalCorrect:  5311 loss:  90.70049273967743 Val_score 0.5985611510791367\n","epoch:  149 totalCorrect:  5314 loss:  90.68857097625732 Val_score 0.5985611510791367\n","epoch:  150 totalCorrect:  5313 loss:  90.67670631408691 Val_score 0.5985611510791367\n","epoch:  151 totalCorrect:  5314 loss:  90.66481447219849 Val_score 0.5990407673860911\n","epoch:  152 totalCorrect:  5316 loss:  90.65283465385437 Val_score 0.5985611510791367\n","epoch:  153 totalCorrect:  5319 loss:  90.64090234041214 Val_score 0.5990407673860911\n","epoch:  154 totalCorrect:  5321 loss:  90.62893980741501 Val_score 0.5990407673860911\n","epoch:  155 totalCorrect:  5321 loss:  90.61690723896027 Val_score 0.5980815347721823\n","epoch:  156 totalCorrect:  5321 loss:  90.60481309890747 Val_score 0.5985611510791367\n","epoch:  157 totalCorrect:  5318 loss:  90.5927854180336 Val_score 0.5980815347721823\n","epoch:  158 totalCorrect:  5318 loss:  90.58074045181274 Val_score 0.5980815347721823\n","epoch:  159 totalCorrect:  5318 loss:  90.56862014532089 Val_score 0.5985611510791367\n","epoch:  160 totalCorrect:  5321 loss:  90.55642926692963 Val_score 0.5995203836930456\n","epoch:  161 totalCorrect:  5320 loss:  90.54413485527039 Val_score 0.6\n","epoch:  162 totalCorrect:  5322 loss:  90.53186368942261 Val_score 0.6\n","epoch:  163 totalCorrect:  5323 loss:  90.51956760883331 Val_score 0.6004796163069545\n","epoch:  164 totalCorrect:  5326 loss:  90.50719857215881 Val_score 0.6004796163069545\n","epoch:  165 totalCorrect:  5329 loss:  90.49474585056305 Val_score 0.6004796163069545\n","epoch:  166 totalCorrect:  5332 loss:  90.48221814632416 Val_score 0.6\n","epoch:  167 totalCorrect:  5335 loss:  90.46964597702026 Val_score 0.5995203836930456\n","epoch:  168 totalCorrect:  5339 loss:  90.45698165893555 Val_score 0.5985611510791367\n","epoch:  169 totalCorrect:  5338 loss:  90.44435560703278 Val_score 0.5976019184652278\n","epoch:  170 totalCorrect:  5339 loss:  90.43162488937378 Val_score 0.5976019184652278\n","epoch:  171 totalCorrect:  5338 loss:  90.41873681545258 Val_score 0.5976019184652278\n","epoch:  172 totalCorrect:  5341 loss:  90.4058609008789 Val_score 0.5976019184652278\n","epoch:  173 totalCorrect:  5344 loss:  90.39294761419296 Val_score 0.5976019184652278\n","epoch:  174 totalCorrect:  5345 loss:  90.37982845306396 Val_score 0.5976019184652278\n","epoch:  175 totalCorrect:  5344 loss:  90.36664259433746 Val_score 0.5980815347721823\n","epoch:  176 totalCorrect:  5344 loss:  90.3535138964653 Val_score 0.5980815347721823\n","epoch:  177 totalCorrect:  5348 loss:  90.34041684865952 Val_score 0.5976019184652278\n","epoch:  178 totalCorrect:  5350 loss:  90.32717907428741 Val_score 0.5971223021582733\n","epoch:  179 totalCorrect:  5351 loss:  90.31400680541992 Val_score 0.5980815347721823\n","epoch:  180 totalCorrect:  5355 loss:  90.30074739456177 Val_score 0.5980815347721823\n","epoch:  181 totalCorrect:  5356 loss:  90.28749281167984 Val_score 0.5980815347721823\n","epoch:  182 totalCorrect:  5358 loss:  90.27402102947235 Val_score 0.5976019184652278\n","epoch:  183 totalCorrect:  5359 loss:  90.2603976726532 Val_score 0.5976019184652278\n","epoch:  184 totalCorrect:  5356 loss:  90.24665427207947 Val_score 0.5976019184652278\n","epoch:  185 totalCorrect:  5363 loss:  90.23297101259232 Val_score 0.5985611510791367\n","epoch:  186 totalCorrect:  5364 loss:  90.21937841176987 Val_score 0.5990407673860911\n","epoch:  187 totalCorrect:  5363 loss:  90.20555490255356 Val_score 0.5985611510791367\n","epoch:  188 totalCorrect:  5362 loss:  90.19179463386536 Val_score 0.5985611510791367\n","epoch:  189 totalCorrect:  5363 loss:  90.178082883358 Val_score 0.5980815347721823\n","epoch:  190 totalCorrect:  5363 loss:  90.16435146331787 Val_score 0.5985611510791367\n","epoch:  191 totalCorrect:  5364 loss:  90.15075671672821 Val_score 0.5985611510791367\n","epoch:  192 totalCorrect:  5365 loss:  90.13707381486893 Val_score 0.5980815347721823\n","epoch:  193 totalCorrect:  5366 loss:  90.12348848581314 Val_score 0.5980815347721823\n","epoch:  194 totalCorrect:  5368 loss:  90.10972249507904 Val_score 0.5980815347721823\n","epoch:  195 totalCorrect:  5367 loss:  90.09588491916656 Val_score 0.5980815347721823\n","epoch:  196 totalCorrect:  5364 loss:  90.08206832408905 Val_score 0.5980815347721823\n","epoch:  197 totalCorrect:  5367 loss:  90.0681049823761 Val_score 0.5985611510791367\n","epoch:  198 totalCorrect:  5368 loss:  90.05412274599075 Val_score 0.5985611510791367\n","epoch:  199 totalCorrect:  5371 loss:  90.04024523496628 Val_score 0.5976019184652278\n","epoch:  200 totalCorrect:  5374 loss:  90.0263482928276 Val_score 0.5980815347721823\n","epoch:  201 totalCorrect:  5377 loss:  90.01231724023819 Val_score 0.5990407673860911\n","epoch:  202 totalCorrect:  5377 loss:  89.998291015625 Val_score 0.5990407673860911\n","epoch:  203 totalCorrect:  5376 loss:  89.98413664102554 Val_score 0.5995203836930456\n","epoch:  204 totalCorrect:  5380 loss:  89.96983295679092 Val_score 0.5995203836930456\n","epoch:  205 totalCorrect:  5385 loss:  89.9555424451828 Val_score 0.6009592326139088\n","epoch:  206 totalCorrect:  5384 loss:  89.94094103574753 Val_score 0.6014388489208633\n","epoch:  207 totalCorrect:  5384 loss:  89.92639523744583 Val_score 0.6014388489208633\n","epoch:  208 totalCorrect:  5387 loss:  89.91187572479248 Val_score 0.6014388489208633\n","epoch:  209 totalCorrect:  5389 loss:  89.89729982614517 Val_score 0.6023980815347721\n","epoch:  210 totalCorrect:  5391 loss:  89.88256055116653 Val_score 0.6028776978417266\n","epoch:  211 totalCorrect:  5393 loss:  89.86780405044556 Val_score 0.6028776978417266\n","epoch:  212 totalCorrect:  5394 loss:  89.85284036397934 Val_score 0.6028776978417266\n","epoch:  213 totalCorrect:  5396 loss:  89.83777087926865 Val_score 0.6028776978417266\n","epoch:  214 totalCorrect:  5402 loss:  89.82257479429245 Val_score 0.6033573141486811\n","epoch:  215 totalCorrect:  5405 loss:  89.8074381351471 Val_score 0.6033573141486811\n","epoch:  216 totalCorrect:  5405 loss:  89.79220741987228 Val_score 0.6038369304556355\n","epoch:  217 totalCorrect:  5408 loss:  89.77687221765518 Val_score 0.6033573141486811\n","epoch:  218 totalCorrect:  5412 loss:  89.76125425100327 Val_score 0.6033573141486811\n","epoch:  219 totalCorrect:  5413 loss:  89.74556070566177 Val_score 0.6033573141486811\n","epoch:  220 totalCorrect:  5412 loss:  89.72998750209808 Val_score 0.6033573141486811\n","epoch:  221 totalCorrect:  5410 loss:  89.7142042517662 Val_score 0.6033573141486811\n","epoch:  222 totalCorrect:  5412 loss:  89.6985986828804 Val_score 0.6033573141486811\n","epoch:  223 totalCorrect:  5417 loss:  89.68274736404419 Val_score 0.6033573141486811\n","epoch:  224 totalCorrect:  5416 loss:  89.66688430309296 Val_score 0.6028776978417266\n","epoch:  225 totalCorrect:  5417 loss:  89.65101146697998 Val_score 0.6023980815347721\n","epoch:  226 totalCorrect:  5415 loss:  89.63523262739182 Val_score 0.6023980815347721\n","epoch:  227 totalCorrect:  5418 loss:  89.61941123008728 Val_score 0.6023980815347721\n","epoch:  228 totalCorrect:  5421 loss:  89.6034465432167 Val_score 0.6028776978417266\n","epoch:  229 totalCorrect:  5427 loss:  89.58754080533981 Val_score 0.6028776978417266\n","epoch:  230 totalCorrect:  5427 loss:  89.57156038284302 Val_score 0.6023980815347721\n","epoch:  231 totalCorrect:  5432 loss:  89.55519676208496 Val_score 0.6023980815347721\n","epoch:  232 totalCorrect:  5439 loss:  89.5386974811554 Val_score 0.6023980815347721\n","epoch:  233 totalCorrect:  5439 loss:  89.52215027809143 Val_score 0.6019184652278178\n","epoch:  234 totalCorrect:  5446 loss:  89.50558704137802 Val_score 0.6019184652278178\n","epoch:  235 totalCorrect:  5447 loss:  89.48889940977097 Val_score 0.6019184652278178\n","epoch:  236 totalCorrect:  5449 loss:  89.47225332260132 Val_score 0.6023980815347721\n","epoch:  237 totalCorrect:  5449 loss:  89.45566898584366 Val_score 0.6023980815347721\n","epoch:  238 totalCorrect:  5450 loss:  89.43899947404861 Val_score 0.6023980815347721\n","epoch:  239 totalCorrect:  5453 loss:  89.42228209972382 Val_score 0.6028776978417266\n","epoch:  240 totalCorrect:  5457 loss:  89.40556287765503 Val_score 0.6019184652278178\n","epoch:  241 totalCorrect:  5460 loss:  89.3888767361641 Val_score 0.6019184652278178\n","epoch:  242 totalCorrect:  5460 loss:  89.37205189466476 Val_score 0.6019184652278178\n","epoch:  243 totalCorrect:  5463 loss:  89.35524475574493 Val_score 0.6019184652278178\n","epoch:  244 totalCorrect:  5464 loss:  89.33832746744156 Val_score 0.6023980815347721\n","epoch:  245 totalCorrect:  5464 loss:  89.3213204741478 Val_score 0.6028776978417266\n","epoch:  246 totalCorrect:  5467 loss:  89.30449277162552 Val_score 0.6028776978417266\n","epoch:  247 totalCorrect:  5471 loss:  89.28741818666458 Val_score 0.6033573141486811\n","epoch:  248 totalCorrect:  5473 loss:  89.27043569087982 Val_score 0.6033573141486811\n","epoch:  249 totalCorrect:  5477 loss:  89.25324958562851 Val_score 0.60431654676259\n","epoch:  250 totalCorrect:  5479 loss:  89.2361211180687 Val_score 0.60431654676259\n","epoch:  251 totalCorrect:  5478 loss:  89.21906226873398 Val_score 0.60431654676259\n","epoch:  252 totalCorrect:  5481 loss:  89.2016926407814 Val_score 0.6047961630695443\n","epoch:  253 totalCorrect:  5482 loss:  89.18435269594193 Val_score 0.6052757793764988\n","epoch:  254 totalCorrect:  5482 loss:  89.16694402694702 Val_score 0.6047961630695443\n","epoch:  255 totalCorrect:  5478 loss:  89.14952450990677 Val_score 0.6047961630695443\n","epoch:  256 totalCorrect:  5478 loss:  89.13190120458603 Val_score 0.6052757793764988\n","epoch:  257 totalCorrect:  5477 loss:  89.11433207988739 Val_score 0.6052757793764988\n","epoch:  258 totalCorrect:  5477 loss:  89.09681516885757 Val_score 0.6067146282973621\n","epoch:  259 totalCorrect:  5479 loss:  89.0789532661438 Val_score 0.6062350119904076\n","epoch:  260 totalCorrect:  5485 loss:  89.061243891716 Val_score 0.6067146282973621\n","epoch:  261 totalCorrect:  5489 loss:  89.04337239265442 Val_score 0.6071942446043166\n","epoch:  262 totalCorrect:  5492 loss:  89.02551120519638 Val_score 0.607673860911271\n","epoch:  263 totalCorrect:  5496 loss:  89.00755381584167 Val_score 0.607673860911271\n","epoch:  264 totalCorrect:  5498 loss:  88.98945736885071 Val_score 0.6081534772182254\n","epoch:  265 totalCorrect:  5499 loss:  88.97150909900665 Val_score 0.6081534772182254\n","epoch:  266 totalCorrect:  5496 loss:  88.95342689752579 Val_score 0.6081534772182254\n","epoch:  267 totalCorrect:  5497 loss:  88.93531382083893 Val_score 0.607673860911271\n","epoch:  268 totalCorrect:  5499 loss:  88.91699349880219 Val_score 0.607673860911271\n","epoch:  269 totalCorrect:  5502 loss:  88.89882171154022 Val_score 0.6067146282973621\n","epoch:  270 totalCorrect:  5505 loss:  88.88051843643188 Val_score 0.6071942446043166\n","epoch:  271 totalCorrect:  5507 loss:  88.86206060647964 Val_score 0.6071942446043166\n","epoch:  272 totalCorrect:  5511 loss:  88.84370368719101 Val_score 0.6071942446043166\n","epoch:  273 totalCorrect:  5514 loss:  88.8251366019249 Val_score 0.6091127098321343\n","epoch:  274 totalCorrect:  5515 loss:  88.80652809143066 Val_score 0.6086330935251798\n","epoch:  275 totalCorrect:  5516 loss:  88.7878947854042 Val_score 0.6086330935251798\n","epoch:  276 totalCorrect:  5516 loss:  88.7690127491951 Val_score 0.6081534772182254\n","epoch:  277 totalCorrect:  5517 loss:  88.75009375810623 Val_score 0.6071942446043166\n","epoch:  278 totalCorrect:  5519 loss:  88.73116731643677 Val_score 0.607673860911271\n","epoch:  279 totalCorrect:  5518 loss:  88.71218651533127 Val_score 0.6071942446043166\n","epoch:  280 totalCorrect:  5519 loss:  88.69303697347641 Val_score 0.6071942446043166\n","epoch:  281 totalCorrect:  5521 loss:  88.67374446988106 Val_score 0.6071942446043166\n","epoch:  282 totalCorrect:  5525 loss:  88.65435436367989 Val_score 0.6071942446043166\n","epoch:  283 totalCorrect:  5527 loss:  88.63518914580345 Val_score 0.607673860911271\n","epoch:  284 totalCorrect:  5528 loss:  88.61586791276932 Val_score 0.6071942446043166\n","epoch:  285 totalCorrect:  5530 loss:  88.59637767076492 Val_score 0.607673860911271\n","epoch:  286 totalCorrect:  5531 loss:  88.57713061571121 Val_score 0.6071942446043166\n","epoch:  287 totalCorrect:  5532 loss:  88.55758574604988 Val_score 0.607673860911271\n","epoch:  288 totalCorrect:  5535 loss:  88.53806459903717 Val_score 0.6071942446043166\n","epoch:  289 totalCorrect:  5535 loss:  88.51857036352158 Val_score 0.6057553956834533\n","epoch:  290 totalCorrect:  5537 loss:  88.49884977936745 Val_score 0.6057553956834533\n","epoch:  291 totalCorrect:  5540 loss:  88.47927331924438 Val_score 0.6057553956834533\n","epoch:  292 totalCorrect:  5545 loss:  88.45948860049248 Val_score 0.6057553956834533\n","epoch:  293 totalCorrect:  5547 loss:  88.43970859050751 Val_score 0.6057553956834533\n","epoch:  294 totalCorrect:  5549 loss:  88.41982737183571 Val_score 0.6057553956834533\n","epoch:  295 totalCorrect:  5550 loss:  88.39998853206635 Val_score 0.6052757793764988\n","epoch:  296 totalCorrect:  5553 loss:  88.38012862205505 Val_score 0.6052757793764988\n","epoch:  297 totalCorrect:  5550 loss:  88.36006781458855 Val_score 0.6052757793764988\n","epoch:  298 totalCorrect:  5553 loss:  88.33992031216621 Val_score 0.6057553956834533\n","epoch:  299 totalCorrect:  5553 loss:  88.31976211071014 Val_score 0.6047961630695443\n","epoch:  300 totalCorrect:  5556 loss:  88.29962205886841 Val_score 0.6057553956834533\n","epoch:  301 totalCorrect:  5558 loss:  88.27941638231277 Val_score 0.6067146282973621\n","epoch:  302 totalCorrect:  5560 loss:  88.25892540812492 Val_score 0.6067146282973621\n","epoch:  303 totalCorrect:  5565 loss:  88.23866945505142 Val_score 0.6067146282973621\n","epoch:  304 totalCorrect:  5571 loss:  88.2180609703064 Val_score 0.6067146282973621\n","epoch:  305 totalCorrect:  5574 loss:  88.19761380553246 Val_score 0.6071942446043166\n","epoch:  306 totalCorrect:  5579 loss:  88.1769748032093 Val_score 0.6071942446043166\n","epoch:  307 totalCorrect:  5579 loss:  88.15632885694504 Val_score 0.6071942446043166\n","epoch:  308 totalCorrect:  5581 loss:  88.1355317234993 Val_score 0.6081534772182254\n","epoch:  309 totalCorrect:  5582 loss:  88.11480045318604 Val_score 0.6086330935251798\n","epoch:  310 totalCorrect:  5580 loss:  88.09405699372292 Val_score 0.6086330935251798\n","epoch:  311 totalCorrect:  5582 loss:  88.07315799593925 Val_score 0.6081534772182254\n","epoch:  312 totalCorrect:  5582 loss:  88.05229392647743 Val_score 0.6086330935251798\n","epoch:  313 totalCorrect:  5584 loss:  88.03133261203766 Val_score 0.6091127098321343\n","epoch:  314 totalCorrect:  5588 loss:  88.01037061214447 Val_score 0.6091127098321343\n","epoch:  315 totalCorrect:  5590 loss:  87.98937660455704 Val_score 0.6086330935251798\n","epoch:  316 totalCorrect:  5593 loss:  87.96822711825371 Val_score 0.6086330935251798\n","epoch:  317 totalCorrect:  5594 loss:  87.94706267118454 Val_score 0.6095923261390888\n","epoch:  318 totalCorrect:  5598 loss:  87.92585179209709 Val_score 0.6095923261390888\n","epoch:  319 totalCorrect:  5601 loss:  87.90464782714844 Val_score 0.6086330935251798\n","epoch:  320 totalCorrect:  5602 loss:  87.88366886973381 Val_score 0.6071942446043166\n","epoch:  321 totalCorrect:  5604 loss:  87.86211222410202 Val_score 0.6071942446043166\n","epoch:  322 totalCorrect:  5606 loss:  87.84071886539459 Val_score 0.607673860911271\n","epoch:  323 totalCorrect:  5610 loss:  87.81924813985825 Val_score 0.607673860911271\n","epoch:  324 totalCorrect:  5613 loss:  87.79770627617836 Val_score 0.6071942446043166\n","epoch:  325 totalCorrect:  5615 loss:  87.77616775035858 Val_score 0.6081534772182254\n","epoch:  326 totalCorrect:  5617 loss:  87.75453287363052 Val_score 0.607673860911271\n","epoch:  327 totalCorrect:  5617 loss:  87.73279124498367 Val_score 0.6081534772182254\n","epoch:  328 totalCorrect:  5618 loss:  87.71108514070511 Val_score 0.607673860911271\n","epoch:  329 totalCorrect:  5620 loss:  87.68930152058601 Val_score 0.6081534772182254\n","epoch:  330 totalCorrect:  5624 loss:  87.6673269867897 Val_score 0.607673860911271\n","epoch:  331 totalCorrect:  5629 loss:  87.64541637897491 Val_score 0.607673860911271\n","epoch:  332 totalCorrect:  5629 loss:  87.62374693155289 Val_score 0.607673860911271\n","epoch:  333 totalCorrect:  5630 loss:  87.60159668326378 Val_score 0.6081534772182254\n","epoch:  334 totalCorrect:  5634 loss:  87.57964593172073 Val_score 0.6086330935251798\n","epoch:  335 totalCorrect:  5637 loss:  87.55727523565292 Val_score 0.6091127098321343\n","epoch:  336 totalCorrect:  5639 loss:  87.53524097800255 Val_score 0.6071942446043166\n","epoch:  337 totalCorrect:  5640 loss:  87.51295486092567 Val_score 0.6071942446043166\n","epoch:  338 totalCorrect:  5638 loss:  87.49068212509155 Val_score 0.6067146282973621\n","epoch:  339 totalCorrect:  5639 loss:  87.4683338701725 Val_score 0.6057553956834533\n","epoch:  340 totalCorrect:  5642 loss:  87.4461227953434 Val_score 0.6071942446043166\n","epoch:  341 totalCorrect:  5646 loss:  87.42348766326904 Val_score 0.6071942446043166\n","epoch:  342 totalCorrect:  5645 loss:  87.40093013644218 Val_score 0.607673860911271\n","epoch:  343 totalCorrect:  5645 loss:  87.37838414311409 Val_score 0.6081534772182254\n","epoch:  344 totalCorrect:  5646 loss:  87.3558540046215 Val_score 0.6081534772182254\n","epoch:  345 totalCorrect:  5649 loss:  87.33322992920876 Val_score 0.6086330935251798\n","epoch:  346 totalCorrect:  5653 loss:  87.31045889854431 Val_score 0.6086330935251798\n","epoch:  347 totalCorrect:  5661 loss:  87.28779292106628 Val_score 0.6086330935251798\n","epoch:  348 totalCorrect:  5661 loss:  87.26517868041992 Val_score 0.6091127098321343\n","epoch:  349 totalCorrect:  5662 loss:  87.24234163761139 Val_score 0.6110311750599521\n","epoch:  350 totalCorrect:  5663 loss:  87.21956011652946 Val_score 0.6115107913669064\n","epoch:  351 totalCorrect:  5668 loss:  87.19671848416328 Val_score 0.6115107913669064\n","epoch:  352 totalCorrect:  5669 loss:  87.17404609918594 Val_score 0.6115107913669064\n","epoch:  353 totalCorrect:  5672 loss:  87.15096837282181 Val_score 0.6110311750599521\n","epoch:  354 totalCorrect:  5677 loss:  87.12812340259552 Val_score 0.6110311750599521\n","epoch:  355 totalCorrect:  5678 loss:  87.10518035292625 Val_score 0.6110311750599521\n","epoch:  356 totalCorrect:  5681 loss:  87.0824185013771 Val_score 0.6105515587529976\n","epoch:  357 totalCorrect:  5682 loss:  87.05941474437714 Val_score 0.6105515587529976\n","epoch:  358 totalCorrect:  5682 loss:  87.03621512651443 Val_score 0.6105515587529976\n","epoch:  359 totalCorrect:  5683 loss:  87.0130854845047 Val_score 0.6110311750599521\n","epoch:  360 totalCorrect:  5687 loss:  86.99017348885536 Val_score 0.6105515587529976\n","epoch:  361 totalCorrect:  5692 loss:  86.96679535508156 Val_score 0.6115107913669064\n","epoch:  362 totalCorrect:  5694 loss:  86.94365459680557 Val_score 0.6110311750599521\n","epoch:  363 totalCorrect:  5699 loss:  86.92026096582413 Val_score 0.6105515587529976\n","epoch:  364 totalCorrect:  5701 loss:  86.89691069722176 Val_score 0.6100719424460431\n","epoch:  365 totalCorrect:  5706 loss:  86.87367716431618 Val_score 0.6105515587529976\n","epoch:  366 totalCorrect:  5705 loss:  86.85030561685562 Val_score 0.6105515587529976\n","epoch:  367 totalCorrect:  5708 loss:  86.82686585187912 Val_score 0.6105515587529976\n","epoch:  368 totalCorrect:  5706 loss:  86.8033598959446 Val_score 0.6105515587529976\n","epoch:  369 totalCorrect:  5708 loss:  86.77988961338997 Val_score 0.6105515587529976\n","epoch:  370 totalCorrect:  5709 loss:  86.75651890039444 Val_score 0.6105515587529976\n","epoch:  371 totalCorrect:  5712 loss:  86.73279637098312 Val_score 0.6105515587529976\n","epoch:  372 totalCorrect:  5714 loss:  86.70909613370895 Val_score 0.6105515587529976\n","epoch:  373 totalCorrect:  5718 loss:  86.685352653265 Val_score 0.6105515587529976\n","epoch:  374 totalCorrect:  5719 loss:  86.66182401776314 Val_score 0.6110311750599521\n","epoch:  375 totalCorrect:  5723 loss:  86.63817220926285 Val_score 0.6110311750599521\n","epoch:  376 totalCorrect:  5728 loss:  86.61438676714897 Val_score 0.6110311750599521\n","epoch:  377 totalCorrect:  5729 loss:  86.59056234359741 Val_score 0.6105515587529976\n","epoch:  378 totalCorrect:  5731 loss:  86.56684404611588 Val_score 0.6105515587529976\n","epoch:  379 totalCorrect:  5734 loss:  86.54273954033852 Val_score 0.6095923261390888\n","epoch:  380 totalCorrect:  5736 loss:  86.51919874548912 Val_score 0.6100719424460431\n","epoch:  381 totalCorrect:  5736 loss:  86.4950256049633 Val_score 0.6095923261390888\n","epoch:  382 totalCorrect:  5741 loss:  86.47123858332634 Val_score 0.6100719424460431\n","epoch:  383 totalCorrect:  5742 loss:  86.44723883271217 Val_score 0.6100719424460431\n","epoch:  384 totalCorrect:  5742 loss:  86.42334187030792 Val_score 0.6100719424460431\n","epoch:  385 totalCorrect:  5744 loss:  86.39927315711975 Val_score 0.6100719424460431\n","epoch:  386 totalCorrect:  5743 loss:  86.37515550851822 Val_score 0.6095923261390888\n","epoch:  387 totalCorrect:  5751 loss:  86.35123145580292 Val_score 0.6100719424460431\n","epoch:  388 totalCorrect:  5752 loss:  86.32701250910759 Val_score 0.6110311750599521\n","epoch:  389 totalCorrect:  5757 loss:  86.30275717377663 Val_score 0.6119904076738609\n","epoch:  390 totalCorrect:  5758 loss:  86.2782194018364 Val_score 0.6119904076738609\n","epoch:  391 totalCorrect:  5763 loss:  86.25383150577545 Val_score 0.6119904076738609\n","epoch:  392 totalCorrect:  5763 loss:  86.22925731539726 Val_score 0.6124700239808153\n","epoch:  393 totalCorrect:  5766 loss:  86.20481064915657 Val_score 0.6124700239808153\n","epoch:  394 totalCorrect:  5766 loss:  86.18028038740158 Val_score 0.6124700239808153\n","epoch:  395 totalCorrect:  5766 loss:  86.15578618645668 Val_score 0.6119904076738609\n","epoch:  396 totalCorrect:  5771 loss:  86.13134080171585 Val_score 0.6119904076738609\n","epoch:  397 totalCorrect:  5770 loss:  86.10699275135994 Val_score 0.6124700239808153\n","epoch:  398 totalCorrect:  5772 loss:  86.08257082104683 Val_score 0.6124700239808153\n","epoch:  399 totalCorrect:  5772 loss:  86.05817157030106 Val_score 0.6139088729016786\n","epoch:  400 totalCorrect:  5773 loss:  86.0334801375866 Val_score 0.6134292565947242\n","epoch:  401 totalCorrect:  5774 loss:  86.00886225700378 Val_score 0.6139088729016786\n","epoch:  402 totalCorrect:  5779 loss:  85.98407563567162 Val_score 0.6143884892086331\n","epoch:  403 totalCorrect:  5788 loss:  85.95943543314934 Val_score 0.6139088729016786\n","epoch:  404 totalCorrect:  5789 loss:  85.93500036001205 Val_score 0.6134292565947242\n","epoch:  405 totalCorrect:  5793 loss:  85.9099440574646 Val_score 0.6143884892086331\n","epoch:  406 totalCorrect:  5794 loss:  85.88553726673126 Val_score 0.6143884892086331\n","epoch:  407 totalCorrect:  5794 loss:  85.86092600226402 Val_score 0.6143884892086331\n","epoch:  408 totalCorrect:  5795 loss:  85.83564418554306 Val_score 0.6139088729016786\n","epoch:  409 totalCorrect:  5800 loss:  85.81146791577339 Val_score 0.6129496402877698\n","epoch:  410 totalCorrect:  5801 loss:  85.78635156154633 Val_score 0.6134292565947242\n","epoch:  411 totalCorrect:  5804 loss:  85.7615818977356 Val_score 0.6129496402877698\n","epoch:  412 totalCorrect:  5807 loss:  85.73661822080612 Val_score 0.6134292565947242\n","epoch:  413 totalCorrect:  5808 loss:  85.71144783496857 Val_score 0.6134292565947242\n","epoch:  414 totalCorrect:  5819 loss:  85.68702155351639 Val_score 0.6124700239808153\n","epoch:  415 totalCorrect:  5821 loss:  85.66175073385239 Val_score 0.6124700239808153\n","epoch:  416 totalCorrect:  5823 loss:  85.63683852553368 Val_score 0.6124700239808153\n","epoch:  417 totalCorrect:  5823 loss:  85.61214065551758 Val_score 0.6124700239808153\n","epoch:  418 totalCorrect:  5827 loss:  85.58757495880127 Val_score 0.6129496402877698\n","epoch:  419 totalCorrect:  5827 loss:  85.5628433227539 Val_score 0.6129496402877698\n","epoch:  420 totalCorrect:  5825 loss:  85.53767409920692 Val_score 0.6124700239808153\n","epoch:  421 totalCorrect:  5825 loss:  85.51279702782631 Val_score 0.6124700239808153\n","epoch:  422 totalCorrect:  5828 loss:  85.48832285404205 Val_score 0.6124700239808153\n","epoch:  423 totalCorrect:  5827 loss:  85.46321493387222 Val_score 0.6105515587529976\n","epoch:  424 totalCorrect:  5832 loss:  85.43794161081314 Val_score 0.6110311750599521\n","epoch:  425 totalCorrect:  5832 loss:  85.41269570589066 Val_score 0.6105515587529976\n","epoch:  426 totalCorrect:  5836 loss:  85.3875949382782 Val_score 0.6105515587529976\n","epoch:  427 totalCorrect:  5839 loss:  85.36257147789001 Val_score 0.6105515587529976\n","epoch:  428 totalCorrect:  5841 loss:  85.33702889084816 Val_score 0.6086330935251798\n","epoch:  429 totalCorrect:  5841 loss:  85.3119795024395 Val_score 0.6095923261390888\n","epoch:  430 totalCorrect:  5841 loss:  85.28645426034927 Val_score 0.6095923261390888\n","epoch:  431 totalCorrect:  5843 loss:  85.26135951280594 Val_score 0.6100719424460431\n","epoch:  432 totalCorrect:  5850 loss:  85.23571902513504 Val_score 0.6100719424460431\n","epoch:  433 totalCorrect:  5854 loss:  85.21011024713516 Val_score 0.6105515587529976\n","epoch:  434 totalCorrect:  5856 loss:  85.1842252612114 Val_score 0.6110311750599521\n","epoch:  435 totalCorrect:  5858 loss:  85.1582995057106 Val_score 0.6105515587529976\n","epoch:  436 totalCorrect:  5860 loss:  85.13292795419693 Val_score 0.6105515587529976\n","epoch:  437 totalCorrect:  5864 loss:  85.1073462665081 Val_score 0.6100719424460431\n","epoch:  438 totalCorrect:  5864 loss:  85.08137062191963 Val_score 0.6091127098321343\n","epoch:  439 totalCorrect:  5865 loss:  85.05550903081894 Val_score 0.6091127098321343\n","epoch:  440 totalCorrect:  5864 loss:  85.0296721458435 Val_score 0.6086330935251798\n","epoch:  441 totalCorrect:  5866 loss:  85.00385555624962 Val_score 0.6081534772182254\n","epoch:  442 totalCorrect:  5866 loss:  84.97805216908455 Val_score 0.607673860911271\n","epoch:  443 totalCorrect:  5870 loss:  84.95235303044319 Val_score 0.607673860911271\n","epoch:  444 totalCorrect:  5874 loss:  84.92602616548538 Val_score 0.607673860911271\n","epoch:  445 totalCorrect:  5877 loss:  84.90016478300095 Val_score 0.607673860911271\n","epoch:  446 totalCorrect:  5880 loss:  84.87411758303642 Val_score 0.607673860911271\n","epoch:  447 totalCorrect:  5883 loss:  84.84838137030602 Val_score 0.6071942446043166\n","epoch:  448 totalCorrect:  5885 loss:  84.82243779301643 Val_score 0.607673860911271\n","epoch:  449 totalCorrect:  5884 loss:  84.79660218954086 Val_score 0.6071942446043166\n","epoch:  450 totalCorrect:  5887 loss:  84.77020335197449 Val_score 0.607673860911271\n","epoch:  451 totalCorrect:  5889 loss:  84.74393039941788 Val_score 0.6071942446043166\n","epoch:  452 totalCorrect:  5895 loss:  84.71781241893768 Val_score 0.607673860911271\n","epoch:  453 totalCorrect:  5899 loss:  84.69150024652481 Val_score 0.6081534772182254\n","epoch:  454 totalCorrect:  5897 loss:  84.66536808013916 Val_score 0.6081534772182254\n","epoch:  455 totalCorrect:  5901 loss:  84.63902151584625 Val_score 0.6081534772182254\n","epoch:  456 totalCorrect:  5903 loss:  84.61282351613045 Val_score 0.607673860911271\n","epoch:  457 totalCorrect:  5904 loss:  84.58675161004066 Val_score 0.607673860911271\n","epoch:  458 totalCorrect:  5905 loss:  84.5604338645935 Val_score 0.607673860911271\n","epoch:  459 totalCorrect:  5906 loss:  84.53417906165123 Val_score 0.6071942446043166\n","epoch:  460 totalCorrect:  5906 loss:  84.50766876339912 Val_score 0.607673860911271\n","epoch:  461 totalCorrect:  5911 loss:  84.48166626691818 Val_score 0.607673860911271\n","epoch:  462 totalCorrect:  5912 loss:  84.45536056160927 Val_score 0.607673860911271\n","epoch:  463 totalCorrect:  5914 loss:  84.4292049407959 Val_score 0.607673860911271\n","epoch:  464 totalCorrect:  5916 loss:  84.40315416455269 Val_score 0.6071942446043166\n","epoch:  465 totalCorrect:  5915 loss:  84.37669956684113 Val_score 0.6071942446043166\n","epoch:  466 totalCorrect:  5917 loss:  84.350750207901 Val_score 0.607673860911271\n","epoch:  467 totalCorrect:  5922 loss:  84.32445213198662 Val_score 0.607673860911271\n","epoch:  468 totalCorrect:  5922 loss:  84.29853647947311 Val_score 0.607673860911271\n","epoch:  469 totalCorrect:  5924 loss:  84.27242878079414 Val_score 0.607673860911271\n","epoch:  470 totalCorrect:  5924 loss:  84.24621221423149 Val_score 0.6071942446043166\n","epoch:  471 totalCorrect:  5921 loss:  84.22023856639862 Val_score 0.6071942446043166\n","epoch:  472 totalCorrect:  5921 loss:  84.19430068135262 Val_score 0.6071942446043166\n","epoch:  473 totalCorrect:  5923 loss:  84.16807505488396 Val_score 0.6067146282973621\n","epoch:  474 totalCorrect:  5922 loss:  84.14182099699974 Val_score 0.6062350119904076\n","epoch:  475 totalCorrect:  5923 loss:  84.11619874835014 Val_score 0.6062350119904076\n","epoch:  476 totalCorrect:  5928 loss:  84.08982676267624 Val_score 0.6062350119904076\n","epoch:  477 totalCorrect:  5926 loss:  84.06414794921875 Val_score 0.6067146282973621\n","epoch:  478 totalCorrect:  5927 loss:  84.03801417350769 Val_score 0.6057553956834533\n","epoch:  479 totalCorrect:  5927 loss:  84.01244524121284 Val_score 0.6057553956834533\n","epoch:  480 totalCorrect:  5932 loss:  83.98599204421043 Val_score 0.6047961630695443\n","epoch:  481 totalCorrect:  5935 loss:  83.95995169878006 Val_score 0.6052757793764988\n","epoch:  482 totalCorrect:  5940 loss:  83.93411442637444 Val_score 0.6052757793764988\n","epoch:  483 totalCorrect:  5938 loss:  83.90793749690056 Val_score 0.6067146282973621\n","epoch:  484 totalCorrect:  5940 loss:  83.88159549236298 Val_score 0.6067146282973621\n","epoch:  485 totalCorrect:  5943 loss:  83.85557967424393 Val_score 0.6062350119904076\n","epoch:  486 totalCorrect:  5946 loss:  83.82955518364906 Val_score 0.6062350119904076\n","epoch:  487 totalCorrect:  5952 loss:  83.80349299311638 Val_score 0.6062350119904076\n","epoch:  488 totalCorrect:  5953 loss:  83.77704086899757 Val_score 0.6057553956834533\n","epoch:  489 totalCorrect:  5955 loss:  83.75111421942711 Val_score 0.6067146282973621\n","epoch:  490 totalCorrect:  5956 loss:  83.72468763589859 Val_score 0.6081534772182254\n","epoch:  491 totalCorrect:  5960 loss:  83.69836223125458 Val_score 0.6067146282973621\n","epoch:  492 totalCorrect:  5961 loss:  83.67243781685829 Val_score 0.6067146282973621\n","epoch:  493 totalCorrect:  5962 loss:  83.64608737826347 Val_score 0.6067146282973621\n","epoch:  494 totalCorrect:  5965 loss:  83.61967352032661 Val_score 0.6071942446043166\n","epoch:  495 totalCorrect:  5964 loss:  83.59324926137924 Val_score 0.607673860911271\n","epoch:  496 totalCorrect:  5967 loss:  83.56742578744888 Val_score 0.607673860911271\n","epoch:  497 totalCorrect:  5968 loss:  83.54100513458252 Val_score 0.607673860911271\n","epoch:  498 totalCorrect:  5965 loss:  83.51508063077927 Val_score 0.6067146282973621\n","epoch:  499 totalCorrect:  5965 loss:  83.4886617064476 Val_score 0.6062350119904076\n","epoch:  500 totalCorrect:  5966 loss:  83.46232551336288 Val_score 0.6062350119904076\n","epoch:  501 totalCorrect:  5966 loss:  83.43614083528519 Val_score 0.6067146282973621\n","epoch:  502 totalCorrect:  5967 loss:  83.40993344783783 Val_score 0.6067146282973621\n","epoch:  503 totalCorrect:  5972 loss:  83.38322499394417 Val_score 0.6067146282973621\n","epoch:  504 totalCorrect:  5974 loss:  83.35692974925041 Val_score 0.607673860911271\n","epoch:  505 totalCorrect:  5978 loss:  83.33047741651535 Val_score 0.607673860911271\n","epoch:  506 totalCorrect:  5976 loss:  83.30452406406403 Val_score 0.607673860911271\n","epoch:  507 totalCorrect:  5978 loss:  83.27801486849785 Val_score 0.607673860911271\n","epoch:  508 totalCorrect:  5979 loss:  83.25157737731934 Val_score 0.6081534772182254\n","epoch:  509 totalCorrect:  5983 loss:  83.22520440816879 Val_score 0.6086330935251798\n","epoch:  510 totalCorrect:  5988 loss:  83.19929322600365 Val_score 0.607673860911271\n","epoch:  511 totalCorrect:  5991 loss:  83.17307734489441 Val_score 0.607673860911271\n","epoch:  512 totalCorrect:  5992 loss:  83.14629900455475 Val_score 0.607673860911271\n","epoch:  513 totalCorrect:  5993 loss:  83.11977100372314 Val_score 0.6071942446043166\n","epoch:  514 totalCorrect:  5994 loss:  83.0938729941845 Val_score 0.6067146282973621\n","epoch:  515 totalCorrect:  5994 loss:  83.06689035892487 Val_score 0.6067146282973621\n","epoch:  516 totalCorrect:  5993 loss:  83.04064407944679 Val_score 0.6071942446043166\n","epoch:  517 totalCorrect:  5995 loss:  83.01418697834015 Val_score 0.6071942446043166\n","epoch:  518 totalCorrect:  5995 loss:  82.98763823509216 Val_score 0.6081534772182254\n","epoch:  519 totalCorrect:  5995 loss:  82.96123093366623 Val_score 0.6086330935251798\n","epoch:  520 totalCorrect:  5995 loss:  82.93487948179245 Val_score 0.6095923261390888\n","epoch:  521 totalCorrect:  5997 loss:  82.90825313329697 Val_score 0.6091127098321343\n","epoch:  522 totalCorrect:  5997 loss:  82.88154524564743 Val_score 0.6086330935251798\n","epoch:  523 totalCorrect:  6000 loss:  82.85564932227135 Val_score 0.6086330935251798\n","epoch:  524 totalCorrect:  6002 loss:  82.82875943183899 Val_score 0.6086330935251798\n","epoch:  525 totalCorrect:  6003 loss:  82.80212786793709 Val_score 0.6086330935251798\n","epoch:  526 totalCorrect:  6005 loss:  82.77589023113251 Val_score 0.6091127098321343\n","epoch:  527 totalCorrect:  6009 loss:  82.74942830204964 Val_score 0.6091127098321343\n","epoch:  528 totalCorrect:  6013 loss:  82.72237107157707 Val_score 0.6086330935251798\n","epoch:  529 totalCorrect:  6013 loss:  82.69627013802528 Val_score 0.6086330935251798\n","epoch:  530 totalCorrect:  6017 loss:  82.6698048710823 Val_score 0.6086330935251798\n","epoch:  531 totalCorrect:  6017 loss:  82.6430202126503 Val_score 0.6086330935251798\n","epoch:  532 totalCorrect:  6021 loss:  82.61625644564629 Val_score 0.607673860911271\n","epoch:  533 totalCorrect:  6023 loss:  82.59011566638947 Val_score 0.607673860911271\n","epoch:  534 totalCorrect:  6025 loss:  82.56384471058846 Val_score 0.6071942446043166\n","epoch:  535 totalCorrect:  6024 loss:  82.53735092282295 Val_score 0.6071942446043166\n","epoch:  536 totalCorrect:  6028 loss:  82.5105741918087 Val_score 0.6071942446043166\n","epoch:  537 totalCorrect:  6032 loss:  82.48445019125938 Val_score 0.6071942446043166\n","epoch:  538 totalCorrect:  6032 loss:  82.45814231038094 Val_score 0.607673860911271\n","epoch:  539 totalCorrect:  6035 loss:  82.43177562952042 Val_score 0.607673860911271\n","epoch:  540 totalCorrect:  6039 loss:  82.40470483899117 Val_score 0.6081534772182254\n","epoch:  541 totalCorrect:  6042 loss:  82.37883937358856 Val_score 0.6071942446043166\n","epoch:  542 totalCorrect:  6047 loss:  82.35158976912498 Val_score 0.6062350119904076\n","epoch:  543 totalCorrect:  6051 loss:  82.32537668943405 Val_score 0.6062350119904076\n","epoch:  544 totalCorrect:  6055 loss:  82.2989069223404 Val_score 0.6062350119904076\n","epoch:  545 totalCorrect:  6054 loss:  82.27156794071198 Val_score 0.6062350119904076\n","epoch:  546 totalCorrect:  6058 loss:  82.2447467148304 Val_score 0.6062350119904076\n","epoch:  547 totalCorrect:  6059 loss:  82.21786266565323 Val_score 0.6057553956834533\n","epoch:  548 totalCorrect:  6062 loss:  82.19129261374474 Val_score 0.6057553956834533\n","epoch:  549 totalCorrect:  6063 loss:  82.16453325748444 Val_score 0.6057553956834533\n","epoch:  550 totalCorrect:  6069 loss:  82.13748598098755 Val_score 0.6057553956834533\n","epoch:  551 totalCorrect:  6070 loss:  82.11074829101562 Val_score 0.6062350119904076\n","epoch:  552 totalCorrect:  6074 loss:  82.08430334925652 Val_score 0.6057553956834533\n","epoch:  553 totalCorrect:  6082 loss:  82.05746501684189 Val_score 0.6067146282973621\n","epoch:  554 totalCorrect:  6083 loss:  82.03075459599495 Val_score 0.6057553956834533\n","epoch:  555 totalCorrect:  6086 loss:  82.00378286838531 Val_score 0.6062350119904076\n","epoch:  556 totalCorrect:  6091 loss:  81.97753715515137 Val_score 0.6062350119904076\n","epoch:  557 totalCorrect:  6097 loss:  81.95066985487938 Val_score 0.6062350119904076\n","epoch:  558 totalCorrect:  6096 loss:  81.92445826530457 Val_score 0.6062350119904076\n","epoch:  559 totalCorrect:  6097 loss:  81.897511780262 Val_score 0.6062350119904076\n","epoch:  560 totalCorrect:  6100 loss:  81.87108418345451 Val_score 0.6062350119904076\n","epoch:  561 totalCorrect:  6102 loss:  81.8446104824543 Val_score 0.6067146282973621\n","epoch:  562 totalCorrect:  6106 loss:  81.8179839849472 Val_score 0.6067146282973621\n","epoch:  563 totalCorrect:  6107 loss:  81.79143112897873 Val_score 0.6057553956834533\n","epoch:  564 totalCorrect:  6108 loss:  81.7650837302208 Val_score 0.6057553956834533\n","epoch:  565 totalCorrect:  6114 loss:  81.73854905366898 Val_score 0.6067146282973621\n","epoch:  566 totalCorrect:  6113 loss:  81.71195110678673 Val_score 0.6067146282973621\n","epoch:  567 totalCorrect:  6116 loss:  81.68553563952446 Val_score 0.6057553956834533\n","epoch:  568 totalCorrect:  6120 loss:  81.65899822115898 Val_score 0.6057553956834533\n","epoch:  569 totalCorrect:  6121 loss:  81.63170078396797 Val_score 0.6057553956834533\n","epoch:  570 totalCorrect:  6124 loss:  81.60591116547585 Val_score 0.6067146282973621\n","epoch:  571 totalCorrect:  6125 loss:  81.57921966910362 Val_score 0.6067146282973621\n","epoch:  572 totalCorrect:  6129 loss:  81.5527073442936 Val_score 0.6067146282973621\n","epoch:  573 totalCorrect:  6131 loss:  81.5262653529644 Val_score 0.6062350119904076\n","epoch:  574 totalCorrect:  6131 loss:  81.49923780560493 Val_score 0.6062350119904076\n","epoch:  575 totalCorrect:  6131 loss:  81.47331833839417 Val_score 0.6057553956834533\n","epoch:  576 totalCorrect:  6133 loss:  81.44694301486015 Val_score 0.6057553956834533\n","epoch:  577 totalCorrect:  6133 loss:  81.42065614461899 Val_score 0.6057553956834533\n","epoch:  578 totalCorrect:  6140 loss:  81.39400452375412 Val_score 0.6057553956834533\n","epoch:  579 totalCorrect:  6141 loss:  81.36757892370224 Val_score 0.6057553956834533\n","epoch:  580 totalCorrect:  6144 loss:  81.34116303920746 Val_score 0.6057553956834533\n","epoch:  581 totalCorrect:  6147 loss:  81.31515449285507 Val_score 0.6052757793764988\n","epoch:  582 totalCorrect:  6148 loss:  81.28877577185631 Val_score 0.6052757793764988\n","epoch:  583 totalCorrect:  6151 loss:  81.26320576667786 Val_score 0.6067146282973621\n","epoch:  584 totalCorrect:  6154 loss:  81.23647350072861 Val_score 0.6067146282973621\n","epoch:  585 totalCorrect:  6155 loss:  81.21071711182594 Val_score 0.6067146282973621\n","epoch:  586 totalCorrect:  6162 loss:  81.18466049432755 Val_score 0.6071942446043166\n","epoch:  587 totalCorrect:  6162 loss:  81.15812864899635 Val_score 0.6062350119904076\n","epoch:  588 totalCorrect:  6163 loss:  81.13273093104362 Val_score 0.6057553956834533\n","epoch:  589 totalCorrect:  6171 loss:  81.10601317882538 Val_score 0.6057553956834533\n","epoch:  590 totalCorrect:  6170 loss:  81.0803779065609 Val_score 0.6057553956834533\n","epoch:  591 totalCorrect:  6168 loss:  81.05417543649673 Val_score 0.6052757793764988\n","epoch:  592 totalCorrect:  6169 loss:  81.02803799510002 Val_score 0.6052757793764988\n","epoch:  593 totalCorrect:  6172 loss:  81.00184628367424 Val_score 0.6052757793764988\n","epoch:  594 totalCorrect:  6174 loss:  80.97604075074196 Val_score 0.6057553956834533\n","epoch:  595 totalCorrect:  6177 loss:  80.95004180073738 Val_score 0.6052757793764988\n","epoch:  596 totalCorrect:  6180 loss:  80.92371189594269 Val_score 0.6052757793764988\n","epoch:  597 totalCorrect:  6181 loss:  80.89781132340431 Val_score 0.6047961630695443\n","epoch:  598 totalCorrect:  6185 loss:  80.87238904833794 Val_score 0.6047961630695443\n","epoch:  599 totalCorrect:  6189 loss:  80.84629732370377 Val_score 0.60431654676259\n","epoch:  600 totalCorrect:  6195 loss:  80.82013893127441 Val_score 0.6038369304556355\n","epoch:  601 totalCorrect:  6196 loss:  80.79413291811943 Val_score 0.6038369304556355\n","epoch:  602 totalCorrect:  6198 loss:  80.76839208602905 Val_score 0.60431654676259\n","epoch:  603 totalCorrect:  6200 loss:  80.74233874678612 Val_score 0.6038369304556355\n","epoch:  604 totalCorrect:  6203 loss:  80.71630388498306 Val_score 0.6033573141486811\n","epoch:  605 totalCorrect:  6206 loss:  80.69070973992348 Val_score 0.6033573141486811\n","epoch:  606 totalCorrect:  6208 loss:  80.66449868679047 Val_score 0.6028776978417266\n","epoch:  607 totalCorrect:  6207 loss:  80.63874572515488 Val_score 0.6023980815347721\n","epoch:  608 totalCorrect:  6212 loss:  80.61331659555435 Val_score 0.6023980815347721\n","epoch:  609 totalCorrect:  6216 loss:  80.58728981018066 Val_score 0.6028776978417266\n","epoch:  610 totalCorrect:  6220 loss:  80.5611441731453 Val_score 0.6033573141486811\n","epoch:  611 totalCorrect:  6222 loss:  80.5356984436512 Val_score 0.6028776978417266\n","epoch:  612 totalCorrect:  6226 loss:  80.50981819629669 Val_score 0.6038369304556355\n","epoch:  613 totalCorrect:  6228 loss:  80.48417493700981 Val_score 0.6033573141486811\n","epoch:  614 totalCorrect:  6230 loss:  80.45857805013657 Val_score 0.6038369304556355\n","epoch:  615 totalCorrect:  6230 loss:  80.4321257174015 Val_score 0.6033573141486811\n","epoch:  616 totalCorrect:  6233 loss:  80.40619572997093 Val_score 0.6038369304556355\n","epoch:  617 totalCorrect:  6236 loss:  80.38030597567558 Val_score 0.6038369304556355\n","epoch:  618 totalCorrect:  6241 loss:  80.35514292120934 Val_score 0.6033573141486811\n","epoch:  619 totalCorrect:  6246 loss:  80.32915845513344 Val_score 0.6033573141486811\n","epoch:  620 totalCorrect:  6246 loss:  80.30346024036407 Val_score 0.6033573141486811\n","epoch:  621 totalCorrect:  6250 loss:  80.27791398763657 Val_score 0.6033573141486811\n","epoch:  622 totalCorrect:  6249 loss:  80.25261494517326 Val_score 0.6038369304556355\n","epoch:  623 totalCorrect:  6248 loss:  80.22681140899658 Val_score 0.6033573141486811\n","epoch:  624 totalCorrect:  6252 loss:  80.200898706913 Val_score 0.6038369304556355\n","epoch:  625 totalCorrect:  6252 loss:  80.17526242136955 Val_score 0.60431654676259\n","epoch:  626 totalCorrect:  6258 loss:  80.14945203065872 Val_score 0.60431654676259\n","epoch:  627 totalCorrect:  6262 loss:  80.1239466369152 Val_score 0.6047961630695443\n","epoch:  628 totalCorrect:  6265 loss:  80.09850698709488 Val_score 0.6047961630695443\n","epoch:  629 totalCorrect:  6265 loss:  80.07280650734901 Val_score 0.60431654676259\n","epoch:  630 totalCorrect:  6267 loss:  80.04745441675186 Val_score 0.6038369304556355\n","epoch:  631 totalCorrect:  6268 loss:  80.02187266945839 Val_score 0.6052757793764988\n","epoch:  632 totalCorrect:  6270 loss:  79.99511542916298 Val_score 0.6047961630695443\n","epoch:  633 totalCorrect:  6268 loss:  79.9635896384716 Val_score 0.6047961630695443\n","epoch:  634 totalCorrect:  6268 loss:  79.9368947148323 Val_score 0.6047961630695443\n","epoch:  635 totalCorrect:  6270 loss:  79.91102710366249 Val_score 0.6047961630695443\n","epoch:  636 totalCorrect:  6273 loss:  79.88498345017433 Val_score 0.6047961630695443\n","epoch:  637 totalCorrect:  6274 loss:  79.85959979891777 Val_score 0.6047961630695443\n","epoch:  638 totalCorrect:  6274 loss:  79.83259278535843 Val_score 0.60431654676259\n","epoch:  639 totalCorrect:  6273 loss:  79.80655291676521 Val_score 0.6047961630695443\n","epoch:  640 totalCorrect:  6276 loss:  79.78042003512383 Val_score 0.6052757793764988\n","epoch:  641 totalCorrect:  6278 loss:  79.75462663173676 Val_score 0.6057553956834533\n","epoch:  642 totalCorrect:  6280 loss:  79.7285887002945 Val_score 0.6057553956834533\n","epoch:  643 totalCorrect:  6282 loss:  79.70314493775368 Val_score 0.6062350119904076\n","epoch:  644 totalCorrect:  6283 loss:  79.67708748579025 Val_score 0.6062350119904076\n","epoch:  645 totalCorrect:  6284 loss:  79.65139850974083 Val_score 0.6062350119904076\n","epoch:  646 totalCorrect:  6284 loss:  79.6252866089344 Val_score 0.6062350119904076\n","epoch:  647 totalCorrect:  6285 loss:  79.59939503669739 Val_score 0.6062350119904076\n","epoch:  648 totalCorrect:  6287 loss:  79.57428830862045 Val_score 0.6062350119904076\n","epoch:  649 totalCorrect:  6288 loss:  79.54888990521431 Val_score 0.6062350119904076\n","epoch:  650 totalCorrect:  6294 loss:  79.52353072166443 Val_score 0.6067146282973621\n","epoch:  651 totalCorrect:  6293 loss:  79.49722483754158 Val_score 0.6071942446043166\n","epoch:  652 totalCorrect:  6295 loss:  79.47153237462044 Val_score 0.6071942446043166\n","epoch:  653 totalCorrect:  6297 loss:  79.44640693068504 Val_score 0.6067146282973621\n","epoch:  654 totalCorrect:  6298 loss:  79.42040964961052 Val_score 0.6071942446043166\n","epoch:  655 totalCorrect:  6300 loss:  79.39469981193542 Val_score 0.6067146282973621\n","epoch:  656 totalCorrect:  6301 loss:  79.3693645298481 Val_score 0.607673860911271\n","epoch:  657 totalCorrect:  6301 loss:  79.34365057945251 Val_score 0.6071942446043166\n","epoch:  658 totalCorrect:  6306 loss:  79.31814566254616 Val_score 0.607673860911271\n","epoch:  659 totalCorrect:  6309 loss:  79.29249495267868 Val_score 0.6086330935251798\n","epoch:  660 totalCorrect:  6309 loss:  79.26731926202774 Val_score 0.6091127098321343\n","epoch:  661 totalCorrect:  6313 loss:  79.24090629816055 Val_score 0.6091127098321343\n","epoch:  662 totalCorrect:  6313 loss:  79.21554282307625 Val_score 0.6086330935251798\n","epoch:  663 totalCorrect:  6314 loss:  79.19034352898598 Val_score 0.6091127098321343\n","epoch:  664 totalCorrect:  6312 loss:  79.1650740802288 Val_score 0.6091127098321343\n","epoch:  665 totalCorrect:  6313 loss:  79.13897159695625 Val_score 0.6091127098321343\n","epoch:  666 totalCorrect:  6314 loss:  79.1137184202671 Val_score 0.6091127098321343\n","epoch:  667 totalCorrect:  6317 loss:  79.08734291791916 Val_score 0.6095923261390888\n","epoch:  668 totalCorrect:  6318 loss:  79.06101897358894 Val_score 0.6095923261390888\n","epoch:  669 totalCorrect:  6319 loss:  79.03545024991035 Val_score 0.6100719424460431\n","epoch:  670 totalCorrect:  6321 loss:  79.00990968942642 Val_score 0.6100719424460431\n","epoch:  671 totalCorrect:  6324 loss:  78.98493039608002 Val_score 0.6095923261390888\n","epoch:  672 totalCorrect:  6325 loss:  78.95946389436722 Val_score 0.6100719424460431\n","epoch:  673 totalCorrect:  6326 loss:  78.93434318900108 Val_score 0.6100719424460431\n","epoch:  674 totalCorrect:  6326 loss:  78.90816187858582 Val_score 0.6095923261390888\n","epoch:  675 totalCorrect:  6329 loss:  78.88349747657776 Val_score 0.6086330935251798\n","epoch:  676 totalCorrect:  6328 loss:  78.85797160863876 Val_score 0.6086330935251798\n","epoch:  677 totalCorrect:  6331 loss:  78.83154040575027 Val_score 0.6086330935251798\n","epoch:  678 totalCorrect:  6330 loss:  78.80632594227791 Val_score 0.6086330935251798\n","epoch:  679 totalCorrect:  6334 loss:  78.7811841070652 Val_score 0.6086330935251798\n","epoch:  680 totalCorrect:  6335 loss:  78.75566902756691 Val_score 0.6086330935251798\n","epoch:  681 totalCorrect:  6337 loss:  78.73023572564125 Val_score 0.6086330935251798\n","epoch:  682 totalCorrect:  6340 loss:  78.70555791258812 Val_score 0.6086330935251798\n","epoch:  683 totalCorrect:  6341 loss:  78.67904016375542 Val_score 0.6086330935251798\n","epoch:  684 totalCorrect:  6342 loss:  78.65388894081116 Val_score 0.6086330935251798\n","epoch:  685 totalCorrect:  6343 loss:  78.62871006131172 Val_score 0.6091127098321343\n","epoch:  686 totalCorrect:  6349 loss:  78.60400640964508 Val_score 0.6091127098321343\n","epoch:  687 totalCorrect:  6349 loss:  78.57849586009979 Val_score 0.6095923261390888\n","epoch:  688 totalCorrect:  6350 loss:  78.55344086885452 Val_score 0.6086330935251798\n","epoch:  689 totalCorrect:  6353 loss:  78.52848061919212 Val_score 0.6086330935251798\n","epoch:  690 totalCorrect:  6354 loss:  78.50355035066605 Val_score 0.6091127098321343\n","epoch:  691 totalCorrect:  6352 loss:  78.47759965062141 Val_score 0.6091127098321343\n","epoch:  692 totalCorrect:  6357 loss:  78.45310401916504 Val_score 0.6095923261390888\n","epoch:  693 totalCorrect:  6357 loss:  78.4273861348629 Val_score 0.6100719424460431\n","epoch:  694 totalCorrect:  6361 loss:  78.40241652727127 Val_score 0.6100719424460431\n","epoch:  695 totalCorrect:  6361 loss:  78.37724441289902 Val_score 0.6100719424460431\n","epoch:  696 totalCorrect:  6360 loss:  78.35164564847946 Val_score 0.6095923261390888\n","epoch:  697 totalCorrect:  6362 loss:  78.32748690247536 Val_score 0.6095923261390888\n","epoch:  698 totalCorrect:  6362 loss:  78.30180764198303 Val_score 0.6095923261390888\n","epoch:  699 totalCorrect:  6362 loss:  78.2772268652916 Val_score 0.6095923261390888\n","epoch:  700 totalCorrect:  6363 loss:  78.25182336568832 Val_score 0.6100719424460431\n","epoch:  701 totalCorrect:  6364 loss:  78.22658944129944 Val_score 0.6100719424460431\n","epoch:  702 totalCorrect:  6366 loss:  78.20130816102028 Val_score 0.6100719424460431\n","epoch:  703 totalCorrect:  6368 loss:  78.17623397707939 Val_score 0.6095923261390888\n","epoch:  704 totalCorrect:  6371 loss:  78.1513661146164 Val_score 0.6095923261390888\n","epoch:  705 totalCorrect:  6375 loss:  78.1267000734806 Val_score 0.6095923261390888\n","epoch:  706 totalCorrect:  6375 loss:  78.1018314063549 Val_score 0.6100719424460431\n","epoch:  707 totalCorrect:  6376 loss:  78.07696652412415 Val_score 0.6095923261390888\n","epoch:  708 totalCorrect:  6381 loss:  78.05164498090744 Val_score 0.6100719424460431\n","epoch:  709 totalCorrect:  6385 loss:  78.02798137068748 Val_score 0.6100719424460431\n","epoch:  710 totalCorrect:  6388 loss:  78.00286716222763 Val_score 0.6095923261390888\n","epoch:  711 totalCorrect:  6391 loss:  77.97743859887123 Val_score 0.6095923261390888\n","epoch:  712 totalCorrect:  6393 loss:  77.95263278484344 Val_score 0.6100719424460431\n","epoch:  713 totalCorrect:  6391 loss:  77.92874965071678 Val_score 0.6095923261390888\n","epoch:  714 totalCorrect:  6393 loss:  77.90332490205765 Val_score 0.6095923261390888\n","epoch:  715 totalCorrect:  6395 loss:  77.8786418735981 Val_score 0.6095923261390888\n","epoch:  716 totalCorrect:  6397 loss:  77.85332241654396 Val_score 0.6100719424460431\n","epoch:  717 totalCorrect:  6398 loss:  77.82872447371483 Val_score 0.6095923261390888\n","epoch:  718 totalCorrect:  6400 loss:  77.80401796102524 Val_score 0.6095923261390888\n","epoch:  719 totalCorrect:  6403 loss:  77.7793719470501 Val_score 0.6095923261390888\n","epoch:  720 totalCorrect:  6410 loss:  77.7544717490673 Val_score 0.6095923261390888\n","epoch:  721 totalCorrect:  6412 loss:  77.72995793819427 Val_score 0.6095923261390888\n","epoch:  722 totalCorrect:  6412 loss:  77.70511668920517 Val_score 0.6095923261390888\n","epoch:  723 totalCorrect:  6414 loss:  77.68098497390747 Val_score 0.6095923261390888\n","epoch:  724 totalCorrect:  6416 loss:  77.65524551272392 Val_score 0.6091127098321343\n","epoch:  725 totalCorrect:  6418 loss:  77.63112446665764 Val_score 0.6095923261390888\n","epoch:  726 totalCorrect:  6417 loss:  77.60691940784454 Val_score 0.6095923261390888\n","epoch:  727 totalCorrect:  6419 loss:  77.58192330598831 Val_score 0.6100719424460431\n","epoch:  728 totalCorrect:  6422 loss:  77.55767667293549 Val_score 0.6095923261390888\n","epoch:  729 totalCorrect:  6424 loss:  77.53258836269379 Val_score 0.6091127098321343\n","epoch:  730 totalCorrect:  6425 loss:  77.50805482268333 Val_score 0.6091127098321343\n","epoch:  731 totalCorrect:  6427 loss:  77.48328590393066 Val_score 0.6086330935251798\n","epoch:  732 totalCorrect:  6431 loss:  77.45886942744255 Val_score 0.6086330935251798\n","epoch:  733 totalCorrect:  6433 loss:  77.43341714143753 Val_score 0.6086330935251798\n","epoch:  734 totalCorrect:  6438 loss:  77.40903347730637 Val_score 0.6086330935251798\n","epoch:  735 totalCorrect:  6440 loss:  77.38458290696144 Val_score 0.6086330935251798\n","epoch:  736 totalCorrect:  6442 loss:  77.35940420627594 Val_score 0.6086330935251798\n","epoch:  737 totalCorrect:  6439 loss:  77.33487930893898 Val_score 0.6091127098321343\n","epoch:  738 totalCorrect:  6440 loss:  77.3094861805439 Val_score 0.6091127098321343\n","epoch:  739 totalCorrect:  6442 loss:  77.28462579846382 Val_score 0.6091127098321343\n","epoch:  740 totalCorrect:  6450 loss:  77.26046246290207 Val_score 0.6095923261390888\n","epoch:  741 totalCorrect:  6451 loss:  77.23599761724472 Val_score 0.6091127098321343\n","epoch:  742 totalCorrect:  6454 loss:  77.21131852269173 Val_score 0.6100719424460431\n","epoch:  743 totalCorrect:  6456 loss:  77.18633168935776 Val_score 0.6100719424460431\n","epoch:  744 totalCorrect:  6459 loss:  77.16192093491554 Val_score 0.6095923261390888\n","epoch:  745 totalCorrect:  6462 loss:  77.13672995567322 Val_score 0.6095923261390888\n","epoch:  746 totalCorrect:  6464 loss:  77.11282533407211 Val_score 0.6100719424460431\n","epoch:  747 totalCorrect:  6466 loss:  77.08781135082245 Val_score 0.6095923261390888\n","epoch:  748 totalCorrect:  6465 loss:  77.0633584856987 Val_score 0.6095923261390888\n","epoch:  749 totalCorrect:  6468 loss:  77.03897643089294 Val_score 0.6100719424460431\n","epoch:  750 totalCorrect:  6469 loss:  77.01508259773254 Val_score 0.6100719424460431\n","epoch:  751 totalCorrect:  6469 loss:  76.98969838023186 Val_score 0.6100719424460431\n","epoch:  752 totalCorrect:  6470 loss:  76.96565809845924 Val_score 0.6100719424460431\n","epoch:  753 totalCorrect:  6471 loss:  76.94095426797867 Val_score 0.6100719424460431\n","epoch:  754 totalCorrect:  6473 loss:  76.91646391153336 Val_score 0.6095923261390888\n","epoch:  755 totalCorrect:  6474 loss:  76.89185208082199 Val_score 0.6100719424460431\n","epoch:  756 totalCorrect:  6475 loss:  76.86647939682007 Val_score 0.6091127098321343\n","epoch:  757 totalCorrect:  6476 loss:  76.84218856692314 Val_score 0.6091127098321343\n","epoch:  758 totalCorrect:  6476 loss:  76.81825083494186 Val_score 0.6086330935251798\n","epoch:  759 totalCorrect:  6480 loss:  76.79330530762672 Val_score 0.607673860911271\n","epoch:  760 totalCorrect:  6481 loss:  76.76922270655632 Val_score 0.607673860911271\n","epoch:  761 totalCorrect:  6483 loss:  76.74449357390404 Val_score 0.607673860911271\n","epoch:  762 totalCorrect:  6483 loss:  76.71941220760345 Val_score 0.6081534772182254\n","epoch:  763 totalCorrect:  6484 loss:  76.69542020559311 Val_score 0.607673860911271\n","epoch:  764 totalCorrect:  6487 loss:  76.66991969943047 Val_score 0.6086330935251798\n","epoch:  765 totalCorrect:  6490 loss:  76.64572650194168 Val_score 0.6081534772182254\n","epoch:  766 totalCorrect:  6489 loss:  76.62062680721283 Val_score 0.6086330935251798\n","epoch:  767 totalCorrect:  6492 loss:  76.59671095013618 Val_score 0.6086330935251798\n","epoch:  768 totalCorrect:  6494 loss:  76.57170778512955 Val_score 0.6081534772182254\n","epoch:  769 totalCorrect:  6495 loss:  76.54698097705841 Val_score 0.607673860911271\n","epoch:  770 totalCorrect:  6497 loss:  76.52148613333702 Val_score 0.6081534772182254\n","epoch:  771 totalCorrect:  6500 loss:  76.49701350927353 Val_score 0.6071942446043166\n","epoch:  772 totalCorrect:  6500 loss:  76.47139394283295 Val_score 0.6067146282973621\n","epoch:  773 totalCorrect:  6502 loss:  76.4478994011879 Val_score 0.6067146282973621\n","epoch:  774 totalCorrect:  6504 loss:  76.42239272594452 Val_score 0.6067146282973621\n","epoch:  775 totalCorrect:  6506 loss:  76.39696669578552 Val_score 0.6062350119904076\n","epoch:  776 totalCorrect:  6508 loss:  76.37240818142891 Val_score 0.6057553956834533\n","epoch:  777 totalCorrect:  6510 loss:  76.34811896085739 Val_score 0.6052757793764988\n","epoch:  778 totalCorrect:  6514 loss:  76.32266598939896 Val_score 0.6057553956834533\n","epoch:  779 totalCorrect:  6516 loss:  76.2983877658844 Val_score 0.6047961630695443\n","epoch:  780 totalCorrect:  6517 loss:  76.2739622592926 Val_score 0.6047961630695443\n","epoch:  781 totalCorrect:  6518 loss:  76.24986666440964 Val_score 0.6038369304556355\n","epoch:  782 totalCorrect:  6519 loss:  76.22421497106552 Val_score 0.6033573141486811\n","epoch:  783 totalCorrect:  6519 loss:  76.19987207651138 Val_score 0.6028776978417266\n","epoch:  784 totalCorrect:  6520 loss:  76.17537879943848 Val_score 0.6028776978417266\n","epoch:  785 totalCorrect:  6523 loss:  76.15043833851814 Val_score 0.6038369304556355\n","epoch:  786 totalCorrect:  6529 loss:  76.12627452611923 Val_score 0.6038369304556355\n","epoch:  787 totalCorrect:  6530 loss:  76.10085859894753 Val_score 0.6038369304556355\n","epoch:  788 totalCorrect:  6533 loss:  76.07686933875084 Val_score 0.60431654676259\n","epoch:  789 totalCorrect:  6538 loss:  76.05210140347481 Val_score 0.60431654676259\n","epoch:  790 totalCorrect:  6537 loss:  76.02658703923225 Val_score 0.60431654676259\n","epoch:  791 totalCorrect:  6538 loss:  76.00104281306267 Val_score 0.60431654676259\n","epoch:  792 totalCorrect:  6539 loss:  75.97666731476784 Val_score 0.60431654676259\n","epoch:  793 totalCorrect:  6540 loss:  75.9519727230072 Val_score 0.60431654676259\n","epoch:  794 totalCorrect:  6543 loss:  75.92731842398643 Val_score 0.60431654676259\n","epoch:  795 totalCorrect:  6547 loss:  75.90228134393692 Val_score 0.6038369304556355\n","epoch:  796 totalCorrect:  6547 loss:  75.8775397837162 Val_score 0.6052757793764988\n","epoch:  797 totalCorrect:  6550 loss:  75.85342556238174 Val_score 0.60431654676259\n","epoch:  798 totalCorrect:  6550 loss:  75.82906916737556 Val_score 0.60431654676259\n","epoch:  799 totalCorrect:  6550 loss:  75.80417862534523 Val_score 0.6038369304556355\n","epoch:  800 totalCorrect:  6550 loss:  75.77922269701958 Val_score 0.6038369304556355\n","epoch:  801 totalCorrect:  6551 loss:  75.75601929426193 Val_score 0.6038369304556355\n","epoch:  802 totalCorrect:  6554 loss:  75.73229226469994 Val_score 0.6033573141486811\n","epoch:  803 totalCorrect:  6557 loss:  75.70720520615578 Val_score 0.6038369304556355\n","epoch:  804 totalCorrect:  6559 loss:  75.68302175402641 Val_score 0.6033573141486811\n","epoch:  805 totalCorrect:  6559 loss:  75.6587940454483 Val_score 0.6038369304556355\n","epoch:  806 totalCorrect:  6563 loss:  75.63348215818405 Val_score 0.6038369304556355\n","epoch:  807 totalCorrect:  6568 loss:  75.6100949048996 Val_score 0.60431654676259\n","epoch:  808 totalCorrect:  6567 loss:  75.5851172208786 Val_score 0.6038369304556355\n","epoch:  809 totalCorrect:  6572 loss:  75.56082585453987 Val_score 0.6033573141486811\n","epoch:  810 totalCorrect:  6573 loss:  75.53662982583046 Val_score 0.6033573141486811\n","epoch:  811 totalCorrect:  6575 loss:  75.51168543100357 Val_score 0.6028776978417266\n","epoch:  812 totalCorrect:  6574 loss:  75.48706743121147 Val_score 0.6028776978417266\n","epoch:  813 totalCorrect:  6577 loss:  75.46288564801216 Val_score 0.6033573141486811\n","epoch:  814 totalCorrect:  6581 loss:  75.43917280435562 Val_score 0.6023980815347721\n","epoch:  815 totalCorrect:  6584 loss:  75.41450464725494 Val_score 0.6023980815347721\n","epoch:  816 totalCorrect:  6583 loss:  75.3902952671051 Val_score 0.6033573141486811\n","epoch:  817 totalCorrect:  6586 loss:  75.36599174141884 Val_score 0.6028776978417266\n","epoch:  818 totalCorrect:  6587 loss:  75.34091159701347 Val_score 0.6023980815347721\n","epoch:  819 totalCorrect:  6587 loss:  75.31616827845573 Val_score 0.6023980815347721\n","epoch:  820 totalCorrect:  6592 loss:  75.29161417484283 Val_score 0.6023980815347721\n","epoch:  821 totalCorrect:  6591 loss:  75.26792365312576 Val_score 0.6023980815347721\n","epoch:  822 totalCorrect:  6593 loss:  75.24306240677834 Val_score 0.6019184652278178\n","epoch:  823 totalCorrect:  6599 loss:  75.21921271085739 Val_score 0.6019184652278178\n","epoch:  824 totalCorrect:  6599 loss:  75.19457247853279 Val_score 0.6023980815347721\n","epoch:  825 totalCorrect:  6599 loss:  75.16998219490051 Val_score 0.6014388489208633\n","epoch:  826 totalCorrect:  6598 loss:  75.14536532759666 Val_score 0.6019184652278178\n","epoch:  827 totalCorrect:  6601 loss:  75.12031996250153 Val_score 0.6014388489208633\n","epoch:  828 totalCorrect:  6603 loss:  75.09567153453827 Val_score 0.6004796163069545\n","epoch:  829 totalCorrect:  6603 loss:  75.0721084177494 Val_score 0.6009592326139088\n","epoch:  830 totalCorrect:  6607 loss:  75.04667410254478 Val_score 0.6009592326139088\n","epoch:  831 totalCorrect:  6610 loss:  75.02275341749191 Val_score 0.6009592326139088\n","epoch:  832 totalCorrect:  6612 loss:  74.99850833415985 Val_score 0.6009592326139088\n","epoch:  833 totalCorrect:  6614 loss:  74.97415840625763 Val_score 0.6\n","epoch:  834 totalCorrect:  6616 loss:  74.94900411367416 Val_score 0.6\n","epoch:  835 totalCorrect:  6615 loss:  74.92418465018272 Val_score 0.6004796163069545\n","epoch:  836 totalCorrect:  6617 loss:  74.90020078420639 Val_score 0.6\n","epoch:  837 totalCorrect:  6617 loss:  74.87594965100288 Val_score 0.6014388489208633\n","epoch:  838 totalCorrect:  6618 loss:  74.85159674286842 Val_score 0.6009592326139088\n","epoch:  839 totalCorrect:  6620 loss:  74.82627022266388 Val_score 0.6009592326139088\n","epoch:  840 totalCorrect:  6623 loss:  74.80265688896179 Val_score 0.6019184652278178\n","epoch:  841 totalCorrect:  6626 loss:  74.77788347005844 Val_score 0.6019184652278178\n","epoch:  842 totalCorrect:  6628 loss:  74.75286772847176 Val_score 0.6019184652278178\n","epoch:  843 totalCorrect:  6628 loss:  74.7277901172638 Val_score 0.6019184652278178\n","epoch:  844 totalCorrect:  6632 loss:  74.70409068465233 Val_score 0.6014388489208633\n","epoch:  845 totalCorrect:  6633 loss:  74.67849063873291 Val_score 0.6014388489208633\n","epoch:  846 totalCorrect:  6634 loss:  74.65358951687813 Val_score 0.6009592326139088\n","epoch:  847 totalCorrect:  6634 loss:  74.62965169548988 Val_score 0.6014388489208633\n","epoch:  848 totalCorrect:  6636 loss:  74.60601416230202 Val_score 0.6009592326139088\n","epoch:  849 totalCorrect:  6637 loss:  74.58063107728958 Val_score 0.6009592326139088\n","epoch:  850 totalCorrect:  6637 loss:  74.55594995617867 Val_score 0.6009592326139088\n","epoch:  851 totalCorrect:  6639 loss:  74.53231206536293 Val_score 0.6009592326139088\n","epoch:  852 totalCorrect:  6640 loss:  74.50755763053894 Val_score 0.6014388489208633\n","epoch:  853 totalCorrect:  6640 loss:  74.48241114616394 Val_score 0.6019184652278178\n","epoch:  854 totalCorrect:  6642 loss:  74.45847365260124 Val_score 0.6014388489208633\n","epoch:  855 totalCorrect:  6641 loss:  74.43481940031052 Val_score 0.6019184652278178\n","epoch:  856 totalCorrect:  6643 loss:  74.40863373875618 Val_score 0.6009592326139088\n","epoch:  857 totalCorrect:  6643 loss:  74.38528162240982 Val_score 0.6014388489208633\n","epoch:  858 totalCorrect:  6644 loss:  74.36085471510887 Val_score 0.6009592326139088\n","epoch:  859 totalCorrect:  6646 loss:  74.33673924207687 Val_score 0.6014388489208633\n","epoch:  860 totalCorrect:  6646 loss:  74.31297007203102 Val_score 0.6028776978417266\n","epoch:  861 totalCorrect:  6647 loss:  74.2890539765358 Val_score 0.6028776978417266\n","epoch:  862 totalCorrect:  6649 loss:  74.2647454738617 Val_score 0.6028776978417266\n","epoch:  863 totalCorrect:  6653 loss:  74.23999163508415 Val_score 0.6028776978417266\n","epoch:  864 totalCorrect:  6652 loss:  74.21710541844368 Val_score 0.6028776978417266\n","epoch:  865 totalCorrect:  6652 loss:  74.1919476389885 Val_score 0.6028776978417266\n","epoch:  866 totalCorrect:  6653 loss:  74.16841384768486 Val_score 0.6033573141486811\n","epoch:  867 totalCorrect:  6654 loss:  74.14341220259666 Val_score 0.6028776978417266\n","epoch:  868 totalCorrect:  6657 loss:  74.12021362781525 Val_score 0.6023980815347721\n","epoch:  869 totalCorrect:  6657 loss:  74.09635666012764 Val_score 0.6019184652278178\n","epoch:  870 totalCorrect:  6661 loss:  74.07301098108292 Val_score 0.6019184652278178\n","epoch:  871 totalCorrect:  6662 loss:  74.04946008324623 Val_score 0.6023980815347721\n","epoch:  872 totalCorrect:  6664 loss:  74.02421224117279 Val_score 0.6028776978417266\n","epoch:  873 totalCorrect:  6669 loss:  74.00109472870827 Val_score 0.6028776978417266\n","epoch:  874 totalCorrect:  6672 loss:  73.97693219780922 Val_score 0.6028776978417266\n","epoch:  875 totalCorrect:  6673 loss:  73.95285776257515 Val_score 0.6019184652278178\n","epoch:  876 totalCorrect:  6674 loss:  73.92832762002945 Val_score 0.6019184652278178\n","epoch:  877 totalCorrect:  6675 loss:  73.9046702682972 Val_score 0.6023980815347721\n","epoch:  878 totalCorrect:  6676 loss:  73.88164210319519 Val_score 0.6028776978417266\n","epoch:  879 totalCorrect:  6679 loss:  73.85818463563919 Val_score 0.6019184652278178\n","epoch:  880 totalCorrect:  6679 loss:  73.83450907468796 Val_score 0.6014388489208633\n","epoch:  881 totalCorrect:  6682 loss:  73.80994629859924 Val_score 0.6019184652278178\n","epoch:  882 totalCorrect:  6680 loss:  73.78647410869598 Val_score 0.6019184652278178\n","epoch:  883 totalCorrect:  6682 loss:  73.76169747114182 Val_score 0.6009592326139088\n","epoch:  884 totalCorrect:  6683 loss:  73.73832246661186 Val_score 0.6019184652278178\n","epoch:  885 totalCorrect:  6684 loss:  73.71548715233803 Val_score 0.6009592326139088\n","epoch:  886 totalCorrect:  6684 loss:  73.69036999344826 Val_score 0.6014388489208633\n","epoch:  887 totalCorrect:  6687 loss:  73.66758209466934 Val_score 0.6009592326139088\n","epoch:  888 totalCorrect:  6692 loss:  73.64317435026169 Val_score 0.6\n","epoch:  889 totalCorrect:  6694 loss:  73.61909312009811 Val_score 0.6004796163069545\n","epoch:  890 totalCorrect:  6695 loss:  73.59451022744179 Val_score 0.5995203836930456\n","epoch:  891 totalCorrect:  6696 loss:  73.56924340128899 Val_score 0.5995203836930456\n","epoch:  892 totalCorrect:  6699 loss:  73.54224088788033 Val_score 0.5990407673860911\n","epoch:  893 totalCorrect:  6700 loss:  73.51547342538834 Val_score 0.5985611510791367\n","epoch:  894 totalCorrect:  6701 loss:  73.48937153816223 Val_score 0.5985611510791367\n","epoch:  895 totalCorrect:  6701 loss:  73.46391090750694 Val_score 0.5976019184652278\n","epoch:  896 totalCorrect:  6704 loss:  73.4382199048996 Val_score 0.5976019184652278\n","epoch:  897 totalCorrect:  6706 loss:  73.41428008675575 Val_score 0.5980815347721823\n","epoch:  898 totalCorrect:  6708 loss:  73.38956052064896 Val_score 0.5971223021582733\n","epoch:  899 totalCorrect:  6709 loss:  73.36489763855934 Val_score 0.5980815347721823\n","epoch:  900 totalCorrect:  6709 loss:  73.34205737709999 Val_score 0.5976019184652278\n","epoch:  901 totalCorrect:  6710 loss:  73.31795272231102 Val_score 0.5976019184652278\n","epoch:  902 totalCorrect:  6711 loss:  73.29445269703865 Val_score 0.5976019184652278\n","epoch:  903 totalCorrect:  6714 loss:  73.27001911401749 Val_score 0.5971223021582733\n","epoch:  904 totalCorrect:  6715 loss:  73.24607452750206 Val_score 0.5980815347721823\n","epoch:  905 totalCorrect:  6716 loss:  73.22169876098633 Val_score 0.5985611510791367\n","epoch:  906 totalCorrect:  6720 loss:  73.1986383497715 Val_score 0.5990407673860911\n","epoch:  907 totalCorrect:  6723 loss:  73.17505300045013 Val_score 0.5990407673860911\n","epoch:  908 totalCorrect:  6726 loss:  73.15198880434036 Val_score 0.5990407673860911\n","epoch:  909 totalCorrect:  6725 loss:  73.12723088264465 Val_score 0.5985611510791367\n","epoch:  910 totalCorrect:  6728 loss:  73.1023445725441 Val_score 0.5985611510791367\n","epoch:  911 totalCorrect:  6730 loss:  73.07898217439651 Val_score 0.6\n","epoch:  912 totalCorrect:  6731 loss:  73.0557718873024 Val_score 0.5995203836930456\n","epoch:  913 totalCorrect:  6732 loss:  73.03137028217316 Val_score 0.5990407673860911\n","epoch:  914 totalCorrect:  6733 loss:  73.00786012411118 Val_score 0.5990407673860911\n","epoch:  915 totalCorrect:  6732 loss:  72.98390483856201 Val_score 0.5990407673860911\n","epoch:  916 totalCorrect:  6731 loss:  72.96006754040718 Val_score 0.5995203836930456\n","epoch:  917 totalCorrect:  6735 loss:  72.93510690331459 Val_score 0.6\n","epoch:  918 totalCorrect:  6734 loss:  72.91232573986053 Val_score 0.5990407673860911\n","epoch:  919 totalCorrect:  6737 loss:  72.88855651021004 Val_score 0.5985611510791367\n","epoch:  920 totalCorrect:  6738 loss:  72.86387285590172 Val_score 0.5985611510791367\n","epoch:  921 totalCorrect:  6741 loss:  72.83974459767342 Val_score 0.5985611510791367\n","epoch:  922 totalCorrect:  6739 loss:  72.81520256400108 Val_score 0.5985611510791367\n","epoch:  923 totalCorrect:  6742 loss:  72.79258048534393 Val_score 0.5985611510791367\n","epoch:  924 totalCorrect:  6743 loss:  72.76767772436142 Val_score 0.5985611510791367\n","epoch:  925 totalCorrect:  6744 loss:  72.7434498667717 Val_score 0.5995203836930456\n","epoch:  926 totalCorrect:  6744 loss:  72.72004172205925 Val_score 0.5985611510791367\n","epoch:  927 totalCorrect:  6747 loss:  72.69620224833488 Val_score 0.5990407673860911\n","epoch:  928 totalCorrect:  6749 loss:  72.67245373129845 Val_score 0.5985611510791367\n","epoch:  929 totalCorrect:  6752 loss:  72.64881685376167 Val_score 0.5990407673860911\n","epoch:  930 totalCorrect:  6754 loss:  72.62373059988022 Val_score 0.5990407673860911\n","epoch:  931 totalCorrect:  6758 loss:  72.60291838645935 Val_score 0.5985611510791367\n","epoch:  932 totalCorrect:  6760 loss:  72.57678285241127 Val_score 0.5985611510791367\n","epoch:  933 totalCorrect:  6761 loss:  72.55409374833107 Val_score 0.5990407673860911\n","epoch:  934 totalCorrect:  6760 loss:  72.53035485744476 Val_score 0.5990407673860911\n","epoch:  935 totalCorrect:  6762 loss:  72.50595265626907 Val_score 0.5990407673860911\n","epoch:  936 totalCorrect:  6763 loss:  72.48164570331573 Val_score 0.5990407673860911\n","epoch:  937 totalCorrect:  6764 loss:  72.46045145392418 Val_score 0.5990407673860911\n","epoch:  938 totalCorrect:  6764 loss:  72.43569931387901 Val_score 0.5990407673860911\n","epoch:  939 totalCorrect:  6765 loss:  72.411302536726 Val_score 0.5990407673860911\n","epoch:  940 totalCorrect:  6768 loss:  72.38750097155571 Val_score 0.5985611510791367\n","epoch:  941 totalCorrect:  6768 loss:  72.36225166916847 Val_score 0.5990407673860911\n","epoch:  942 totalCorrect:  6771 loss:  72.33912396430969 Val_score 0.5985611510791367\n","epoch:  943 totalCorrect:  6769 loss:  72.31630617380142 Val_score 0.5985611510791367\n","epoch:  944 totalCorrect:  6770 loss:  72.29095286130905 Val_score 0.5980815347721823\n","epoch:  945 totalCorrect:  6771 loss:  72.2673554122448 Val_score 0.5980815347721823\n","epoch:  946 totalCorrect:  6773 loss:  72.24320313334465 Val_score 0.5980815347721823\n","epoch:  947 totalCorrect:  6773 loss:  72.21929949522018 Val_score 0.5980815347721823\n","epoch:  948 totalCorrect:  6776 loss:  72.1955831348896 Val_score 0.5980815347721823\n","epoch:  949 totalCorrect:  6779 loss:  72.17203918099403 Val_score 0.5985611510791367\n","epoch:  950 totalCorrect:  6781 loss:  72.14697754383087 Val_score 0.5980815347721823\n","epoch:  951 totalCorrect:  6784 loss:  72.12534180283546 Val_score 0.5985611510791367\n","epoch:  952 totalCorrect:  6786 loss:  72.10005924105644 Val_score 0.5990407673860911\n","epoch:  953 totalCorrect:  6789 loss:  72.0776307284832 Val_score 0.5990407673860911\n","epoch:  954 totalCorrect:  6789 loss:  72.05305752158165 Val_score 0.5990407673860911\n","epoch:  955 totalCorrect:  6791 loss:  72.02911880612373 Val_score 0.5990407673860911\n","epoch:  956 totalCorrect:  6791 loss:  72.0055265724659 Val_score 0.5990407673860911\n","epoch:  957 totalCorrect:  6792 loss:  71.98096024990082 Val_score 0.5990407673860911\n","epoch:  958 totalCorrect:  6796 loss:  71.95733299851418 Val_score 0.5990407673860911\n","epoch:  959 totalCorrect:  6798 loss:  71.93340054154396 Val_score 0.5990407673860911\n","epoch:  960 totalCorrect:  6799 loss:  71.91014128923416 Val_score 0.5990407673860911\n","epoch:  961 totalCorrect:  6799 loss:  71.88574874401093 Val_score 0.5990407673860911\n","epoch:  962 totalCorrect:  6800 loss:  71.8636183142662 Val_score 0.5990407673860911\n","epoch:  963 totalCorrect:  6802 loss:  71.83829507231712 Val_score 0.5990407673860911\n","epoch:  964 totalCorrect:  6803 loss:  71.81520381569862 Val_score 0.5995203836930456\n","epoch:  965 totalCorrect:  6806 loss:  71.7890006005764 Val_score 0.6004796163069545\n","epoch:  966 totalCorrect:  6807 loss:  71.76732391119003 Val_score 0.6004796163069545\n","epoch:  967 totalCorrect:  6809 loss:  71.74253207445145 Val_score 0.6\n","epoch:  968 totalCorrect:  6811 loss:  71.71908044815063 Val_score 0.6\n","epoch:  969 totalCorrect:  6814 loss:  71.69348815083504 Val_score 0.6\n","epoch:  970 totalCorrect:  6812 loss:  71.66991186141968 Val_score 0.6\n","epoch:  971 totalCorrect:  6812 loss:  71.64627134799957 Val_score 0.6004796163069545\n","epoch:  972 totalCorrect:  6819 loss:  71.62206336855888 Val_score 0.6004796163069545\n","epoch:  973 totalCorrect:  6820 loss:  71.59860855340958 Val_score 0.6\n","epoch:  974 totalCorrect:  6823 loss:  71.57552856206894 Val_score 0.6009592326139088\n","epoch:  975 totalCorrect:  6826 loss:  71.55190572142601 Val_score 0.6004796163069545\n","epoch:  976 totalCorrect:  6829 loss:  71.5285034775734 Val_score 0.6014388489208633\n","epoch:  977 totalCorrect:  6832 loss:  71.504567861557 Val_score 0.6004796163069545\n","epoch:  978 totalCorrect:  6832 loss:  71.47957187891006 Val_score 0.6009592326139088\n","epoch:  979 totalCorrect:  6835 loss:  71.45573341846466 Val_score 0.6009592326139088\n","epoch:  980 totalCorrect:  6835 loss:  71.43211197853088 Val_score 0.6009592326139088\n","epoch:  981 totalCorrect:  6838 loss:  71.409648925066 Val_score 0.6014388489208633\n","epoch:  982 totalCorrect:  6837 loss:  71.38572442531586 Val_score 0.6014388489208633\n","epoch:  983 totalCorrect:  6841 loss:  71.36149463057518 Val_score 0.6014388489208633\n","epoch:  984 totalCorrect:  6840 loss:  71.33754789829254 Val_score 0.6009592326139088\n","epoch:  985 totalCorrect:  6842 loss:  71.31499165296555 Val_score 0.6004796163069545\n","epoch:  986 totalCorrect:  6845 loss:  71.2905909717083 Val_score 0.6009592326139088\n","epoch:  987 totalCorrect:  6845 loss:  71.26747077703476 Val_score 0.6009592326139088\n","epoch:  988 totalCorrect:  6846 loss:  71.24282690882683 Val_score 0.6004796163069545\n","epoch:  989 totalCorrect:  6847 loss:  71.21851482987404 Val_score 0.6009592326139088\n","epoch:  990 totalCorrect:  6848 loss:  71.1974211037159 Val_score 0.6004796163069545\n","epoch:  991 totalCorrect:  6852 loss:  71.17176449298859 Val_score 0.6014388489208633\n","epoch:  992 totalCorrect:  6851 loss:  71.14806351065636 Val_score 0.6\n","epoch:  993 totalCorrect:  6854 loss:  71.12424153089523 Val_score 0.6004796163069545\n","epoch:  994 totalCorrect:  6853 loss:  71.10209548473358 Val_score 0.6004796163069545\n","epoch:  995 totalCorrect:  6859 loss:  71.07731446623802 Val_score 0.6009592326139088\n","epoch:  996 totalCorrect:  6857 loss:  71.05308610200882 Val_score 0.6009592326139088\n","epoch:  997 totalCorrect:  6861 loss:  71.02912670373917 Val_score 0.6014388489208633\n","epoch:  998 totalCorrect:  6862 loss:  71.00601530075073 Val_score 0.6004796163069545\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zbG0JngjPDB_","colab_type":"code","colab":{}},"source":["# pred1 = network(data1.float())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mrzGJmxmudST","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"outputId":"379a1fa2-3cf1-45c1-dc51-581e8c372b0f","executionInfo":{"status":"ok","timestamp":1574101979190,"user_tz":-330,"elapsed":37937,"user":{"displayName":"Srihari Vemuru","photoUrl":"","userId":"13401425132614860256"}}},"source":["# pred1"],"execution_count":166,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.4520, 0.5480],\n","        [0.8856, 0.1144],\n","        [0.6965, 0.3035],\n","        ...,\n","        [0.4767, 0.5233],\n","        [0.0888, 0.9112],\n","        [0.4900, 0.5100]], grad_fn=<SoftmaxBackward>)"]},"metadata":{"tags":[]},"execution_count":166}]},{"cell_type":"code","metadata":{"id":"5bX9bkI8PgM5","colab_type":"code","colab":{}},"source":["# ans=[]\n","# for t in pred1:\n","#   x, y = t\n","#   if (x>0.39):\n","#     ans.append(0)\n","#   else:\n","#     ans.append(1)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9-BSkHzlQAiX","colab_type":"code","outputId":"20d620a5-ae7b-4c67-8aa1-16ecb32fb9c4","executionInfo":{"status":"ok","timestamp":1574101979194,"user_tz":-330,"elapsed":37334,"user":{"displayName":"Srihari Vemuru","photoUrl":"","userId":"13401425132614860256"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# score =accuracy_score(y_val,ans)\n","# print(score)"],"execution_count":168,"outputs":[{"output_type":"stream","text":["0.592326139088729\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jKlwtXU3QVjP","colab_type":"code","colab":{}},"source":["tst = pd.read_csv('/content/gdrive/My Drive/ML_datasets/test_rev.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GpQrnGSxg_oN","colab_type":"code","outputId":"897474fd-5f19-4e36-ccb6-474bf8ab89cf","executionInfo":{"status":"ok","timestamp":1574092791781,"user_tz":-330,"elapsed":1483,"user":{"displayName":"Srihari Vemuru","photoUrl":"","userId":"13401425132614860256"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["tst.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5100, 148)"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"MpMZVwcPhwhd","colab_type":"code","outputId":"e0eaf5a1-55cc-43d3-bf63-069bda0c1c2b","executionInfo":{"status":"ok","timestamp":1574092780329,"user_tz":-330,"elapsed":1144,"user":{"displayName":"Srihari Vemuru","photoUrl":"","userId":"13401425132614860256"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["for i in train.columns:\n","  if(i not in tst.columns):\n","    print(i)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["RtpStateBitfield_1.0\n","OsSuite_400\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fEzLylJPiMyy","colab_type":"code","colab":{}},"source":["tst['RtpStateBitfield_1.0'] = np.zeros(5100)\n","tst['OsSuite_400'] = np.zeros(5100)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"killqSg2flhp","colab_type":"code","colab":{}},"source":["mid = tst['id']\n","tst.drop(columns = ['id'], inplace = True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GLlVl-jmgg6C","colab_type":"code","colab":{}},"source":["a = np.zeros(5100)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VNHD7edOgrNO","colab_type":"code","outputId":"844794b7-3877-4920-f9a7-dc4d01777b93","executionInfo":{"status":"ok","timestamp":1574092801641,"user_tz":-330,"elapsed":2170,"user":{"displayName":"Srihari Vemuru","photoUrl":"","userId":"13401425132614860256"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["a"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 0., ..., 0., 0., 0.])"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"dmbjj64afvod","colab_type":"code","colab":{}},"source":["dset3 = SystemData(tst.values, a)\n","dloader3 = torch.utils.data.DataLoader(dset3, batch_size=tst.shape[0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ls20GUYSgu9X","colab_type":"code","outputId":"e2878267-321b-4ef3-d4ba-e01daecc8781","executionInfo":{"status":"ok","timestamp":1574092813164,"user_tz":-330,"elapsed":2301,"user":{"displayName":"Srihari Vemuru","photoUrl":"","userId":"13401425132614860256"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["batches3 = iter(dloader3)\n","data4, labels4 = next(batches3)\n","data4.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([5100, 147])"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"F3HVCfHwPp1g","colab_type":"code","outputId":"fc2ed0a0-b5f6-4569-9e2a-e6d428105389","executionInfo":{"status":"ok","timestamp":1574092847246,"user_tz":-330,"elapsed":1522,"user":{"displayName":"Srihari Vemuru","photoUrl":"","userId":"13401425132614860256"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["data4"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.0000e+00, 6.6326e-01, 1.0000e+00,  ..., 1.2960e+06, 0.0000e+00, 0.0000e+00],\n","        [0.0000e+00, 6.6326e-01, 1.0000e+00,  ..., 1.0491e+06, 0.0000e+00, 0.0000e+00],\n","        [0.0000e+00, 6.6326e-01, 1.0000e+00,  ..., 2.0736e+06, 0.0000e+00, 0.0000e+00],\n","        ...,\n","        [0.0000e+00, 6.6326e-01, 1.0000e+00,  ..., 2.0736e+06, 0.0000e+00, 0.0000e+00],\n","        [0.0000e+00, 5.5862e-02, 2.0000e+00,  ..., 1.0491e+06, 0.0000e+00, 0.0000e+00],\n","        [0.0000e+00, 1.2785e-02, 2.0000e+00,  ..., 1.0491e+06, 0.0000e+00, 0.0000e+00]], dtype=torch.float64)"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"dIjXcfY4inom","colab_type":"code","colab":{}},"source":["pred6 = network(data4.float())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BdDOs0SD_zOF","colab_type":"code","outputId":"597f599a-03ab-4b57-ceb2-bdc6935e5b1c","executionInfo":{"status":"ok","timestamp":1574092977568,"user_tz":-330,"elapsed":1543,"user":{"displayName":"Srihari Vemuru","photoUrl":"","userId":"13401425132614860256"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["s = 0\n","for i in pred6:\n","  if(i[1] > 0):\n","    print(i[1])\n","\n","s"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(0.0147, grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(5.0414e-32, grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1.3873e-43, grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(6.3014e-18, grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(3.6817e-18, grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(2.0319e-43, grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1.3014e-18, grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1.3669e-40, grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(5.5237e-20, grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(2.8670e-19, grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(3.7287e-13, grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n","tensor(1., grad_fn=<SelectBackward>)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"yOajUvkDiwXj","colab_type":"code","colab":{}},"source":["ans5=[]\n","for t in pred6:\n","  x, y = t\n","  # if (x>0.39):\n","  #   ans1.append(0)\n","  # else:\n","  #   ans1.append(1)\n","  ans5.append(y.item())\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rM6JQql7_r1q","colab_type":"code","colab":{}},"source":["ans5 = np.array(ans5, dtype='float')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NFQIA12NQ-KT","colab_type":"code","outputId":"a5c27569-70e2-44b6-9e4b-c81820a79f86","executionInfo":{"status":"ok","timestamp":1574093213476,"user_tz":-330,"elapsed":1777,"user":{"displayName":"Srihari Vemuru","photoUrl":"","userId":"13401425132614860256"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["for i in ans1:\n","  print(i)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.014677110128104687\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","5.041354377315053e-32\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.3872854796815689e-43\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","6.301383324270772e-18\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","3.681682471980632e-18\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","2.0318827732709848e-43\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","1.3013961232968952e-18\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.366854548071713e-40\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","5.523671731263596e-20\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","2.8670333355150534e-19\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","3.7287160121676766e-13\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","1.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mWnxucvPi4Zt","colab_type":"code","outputId":"ffbe444d-f3ad-41fc-9c1a-818b206e16ca","executionInfo":{"status":"ok","timestamp":1574014059943,"user_tz":-330,"elapsed":1516,"user":{"displayName":"Srihari Vemuru","photoUrl":"","userId":"13401425132614860256"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["s = 0\n","for i in ans1:\n","  if i == 1:\n","    s += 1\n","s"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["38"]},"metadata":{"tags":[]},"execution_count":315}]},{"cell_type":"code","metadata":{"id":"t_tx3RwVjLAk","colab_type":"code","colab":{}},"source":["fsdf = {}\n","fsdf['MachineIdentifier'] = mid\n","fsdf['HasDetections'] = ans5\n","\n","daf = pd.DataFrame(fsdf)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F57T_2i1jkRY","colab_type":"code","outputId":"20ea989b-16a4-42b7-a93f-4ca7ec0d5bba","executionInfo":{"status":"ok","timestamp":1574093262399,"user_tz":-330,"elapsed":1612,"user":{"displayName":"Srihari Vemuru","photoUrl":"","userId":"13401425132614860256"}},"colab":{"base_uri":"https://localhost:8080/","height":206}},"source":["daf.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>MachineIdentifier</th>\n","      <th>HasDetections</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0000028988387b115f69f31a3bf04f09</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>000007535c3f730efa9ea0b7ef1bd645</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>000007905a28d863f6d0d597892cd692</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>00000b11598a75ea8ba1beea8459149f</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>000014a5f00daa18e76b81417eeb99fc</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                  MachineIdentifier  HasDetections\n","0  0000028988387b115f69f31a3bf04f09            0.0\n","1  000007535c3f730efa9ea0b7ef1bd645            0.0\n","2  000007905a28d863f6d0d597892cd692            0.0\n","3  00000b11598a75ea8ba1beea8459149f            0.0\n","4  000014a5f00daa18e76b81417eeb99fc            0.0"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"code","metadata":{"id":"nZx-TXhPRT8d","colab_type":"code","colab":{}},"source":["daf2 = daf.iloc[0:2087]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dDNkOrt4RgOA","colab_type":"code","outputId":"e7efddd0-8e1b-4656-d8b2-5fb585ddbd2d","executionInfo":{"status":"ok","timestamp":1574093676564,"user_tz":-330,"elapsed":1578,"user":{"displayName":"Srihari Vemuru","photoUrl":"","userId":"13401425132614860256"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["daf2.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2087, 2)"]},"metadata":{"tags":[]},"execution_count":60}]},{"cell_type":"code","metadata":{"id":"M1HTbjYtjvXs","colab_type":"code","colab":{}},"source":["daf2.to_csv('/content/gdrive/My Drive/ML_datasets/sub1.csv', index=False)"],"execution_count":0,"outputs":[]}]}